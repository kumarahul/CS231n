{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.367464\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.783567 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: 0.177951 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: -1.532883 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: -3.682765 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: 2.115122 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: 2.150524 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: -1.794539 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: -0.960505 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: 1.817252 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: -0.637231 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: -0.340937 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: -0.859406 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: 2.083052 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: 0.885340 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: 0.699260 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: 1.941793 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: 1.814966 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: 2.313450 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: -1.574385 analytic: 0.000000, relative error: 1.000000e+00\n",
      "numerical: 1.753156 analytic: 0.000000, relative error: 1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.367464e+00 computed in 0.015815s\n",
      "vectorized loss: 2.367464e+00 computed in 0.015734s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 6.643501\n",
      "iteration 100 / 1500: loss 6.327861\n",
      "iteration 200 / 1500: loss 6.411605\n",
      "iteration 300 / 1500: loss 5.958871\n",
      "iteration 400 / 1500: loss 6.224027\n",
      "iteration 500 / 1500: loss 6.762670\n",
      "iteration 600 / 1500: loss 6.398483\n",
      "iteration 700 / 1500: loss 6.041568\n",
      "iteration 800 / 1500: loss 5.927039\n",
      "iteration 900 / 1500: loss 6.265608\n",
      "iteration 1000 / 1500: loss 6.061321\n",
      "iteration 1100 / 1500: loss 6.239294\n",
      "iteration 1200 / 1500: loss 6.527225\n",
      "iteration 1300 / 1500: loss 6.031186\n",
      "iteration 1400 / 1500: loss 6.020359\n",
      "training accuracy: 0.081866\n",
      "validation accuracy: 0.077551\n",
      "Got 1140 / 14700 correct => valid accuracy: 0.077551\n",
      "iteration 0 / 1500: loss 4.804597\n",
      "iteration 100 / 1500: loss 5.018026\n",
      "iteration 200 / 1500: loss 5.298487\n",
      "iteration 300 / 1500: loss 5.216493\n",
      "iteration 400 / 1500: loss 5.112262\n",
      "iteration 500 / 1500: loss 5.422331\n",
      "iteration 600 / 1500: loss 4.864354\n",
      "iteration 700 / 1500: loss 5.011653\n",
      "iteration 800 / 1500: loss 4.952856\n",
      "iteration 900 / 1500: loss 4.921230\n",
      "iteration 1000 / 1500: loss 5.011869\n",
      "iteration 1100 / 1500: loss 4.567567\n",
      "iteration 1200 / 1500: loss 4.983486\n",
      "iteration 1300 / 1500: loss 5.086282\n",
      "iteration 1400 / 1500: loss 4.811630\n",
      "training accuracy: 0.109504\n",
      "validation accuracy: 0.108980\n",
      "Got 1602 / 14700 correct => valid accuracy: 0.108980\n",
      "iteration 0 / 1500: loss 5.579406\n",
      "iteration 100 / 1500: loss 5.192580\n",
      "iteration 200 / 1500: loss 5.707571\n",
      "iteration 300 / 1500: loss 5.717953\n",
      "iteration 400 / 1500: loss 5.688618\n",
      "iteration 500 / 1500: loss 5.392453\n",
      "iteration 600 / 1500: loss 5.790236\n",
      "iteration 700 / 1500: loss 6.016133\n",
      "iteration 800 / 1500: loss 5.784714\n",
      "iteration 900 / 1500: loss 5.627790\n",
      "iteration 1000 / 1500: loss 5.677087\n",
      "iteration 1100 / 1500: loss 6.063322\n",
      "iteration 1200 / 1500: loss 5.913590\n",
      "iteration 1300 / 1500: loss 5.582277\n",
      "iteration 1400 / 1500: loss 5.667040\n",
      "training accuracy: 0.085656\n",
      "validation accuracy: 0.082721\n",
      "Got 1216 / 14700 correct => valid accuracy: 0.082721\n",
      "iteration 0 / 1500: loss 6.021829\n",
      "iteration 100 / 1500: loss 5.630709\n",
      "iteration 200 / 1500: loss 6.107796\n",
      "iteration 300 / 1500: loss 5.690946\n",
      "iteration 400 / 1500: loss 6.123217\n",
      "iteration 500 / 1500: loss 5.836058\n",
      "iteration 600 / 1500: loss 5.842148\n",
      "iteration 700 / 1500: loss 5.681216\n",
      "iteration 800 / 1500: loss 5.864137\n",
      "iteration 900 / 1500: loss 5.542180\n",
      "iteration 1000 / 1500: loss 5.771542\n",
      "iteration 1100 / 1500: loss 5.782872\n",
      "iteration 1200 / 1500: loss 5.705372\n",
      "iteration 1300 / 1500: loss 5.661812\n",
      "iteration 1400 / 1500: loss 5.820954\n",
      "training accuracy: 0.098717\n",
      "validation accuracy: 0.095918\n",
      "Got 1410 / 14700 correct => valid accuracy: 0.095918\n",
      "iteration 0 / 1500: loss 6.339290\n",
      "iteration 100 / 1500: loss 5.641661\n",
      "iteration 200 / 1500: loss 6.042236\n",
      "iteration 300 / 1500: loss 5.773814\n",
      "iteration 400 / 1500: loss 6.499792\n",
      "iteration 500 / 1500: loss 6.152614\n",
      "iteration 600 / 1500: loss 5.946200\n",
      "iteration 700 / 1500: loss 5.817936\n",
      "iteration 800 / 1500: loss 6.042385\n",
      "iteration 900 / 1500: loss 6.271121\n",
      "iteration 1000 / 1500: loss 5.755342\n",
      "iteration 1100 / 1500: loss 6.549337\n",
      "iteration 1200 / 1500: loss 5.722589\n",
      "iteration 1300 / 1500: loss 6.012680\n",
      "iteration 1400 / 1500: loss 6.136135\n",
      "training accuracy: 0.101691\n",
      "validation accuracy: 0.101429\n",
      "Got 1491 / 14700 correct => valid accuracy: 0.101429\n",
      "iteration 0 / 1500: loss 5.774253\n",
      "iteration 100 / 1500: loss 5.569719\n",
      "iteration 200 / 1500: loss 5.649992\n",
      "iteration 300 / 1500: loss 5.323038\n",
      "iteration 400 / 1500: loss 5.614918\n",
      "iteration 500 / 1500: loss 5.724107\n",
      "iteration 600 / 1500: loss 5.238612\n",
      "iteration 700 / 1500: loss 5.497517\n",
      "iteration 800 / 1500: loss 5.953768\n",
      "iteration 900 / 1500: loss 5.778693\n",
      "iteration 1000 / 1500: loss 6.272314\n",
      "iteration 1100 / 1500: loss 5.951012\n",
      "iteration 1200 / 1500: loss 5.737388\n",
      "iteration 1300 / 1500: loss 5.453594\n",
      "iteration 1400 / 1500: loss 5.808479\n",
      "training accuracy: 0.113294\n",
      "validation accuracy: 0.108163\n",
      "Got 1590 / 14700 correct => valid accuracy: 0.108163\n",
      "iteration 0 / 1500: loss 6.548970\n",
      "iteration 100 / 1500: loss 5.784973\n",
      "iteration 200 / 1500: loss 5.310630\n",
      "iteration 300 / 1500: loss 5.951016\n",
      "iteration 400 / 1500: loss 6.160344\n",
      "iteration 500 / 1500: loss 5.995076\n",
      "iteration 600 / 1500: loss 5.531275\n",
      "iteration 700 / 1500: loss 5.603824\n",
      "iteration 800 / 1500: loss 5.625797\n",
      "iteration 900 / 1500: loss 6.060993\n",
      "iteration 1000 / 1500: loss 6.225388\n",
      "iteration 1100 / 1500: loss 6.172804\n",
      "iteration 1200 / 1500: loss 5.769502\n",
      "iteration 1300 / 1500: loss 6.186656\n",
      "iteration 1400 / 1500: loss 5.637098\n",
      "training accuracy: 0.123615\n",
      "validation accuracy: 0.124014\n",
      "Got 1823 / 14700 correct => valid accuracy: 0.124014\n",
      "iteration 0 / 1500: loss 6.739762\n",
      "iteration 100 / 1500: loss 6.388662\n",
      "iteration 200 / 1500: loss 6.695208\n",
      "iteration 300 / 1500: loss 6.647727\n",
      "iteration 400 / 1500: loss 6.726618\n",
      "iteration 500 / 1500: loss 7.037091\n",
      "iteration 600 / 1500: loss 6.488205\n",
      "iteration 700 / 1500: loss 6.461144\n",
      "iteration 800 / 1500: loss 6.618706\n",
      "iteration 900 / 1500: loss 6.424252\n",
      "iteration 1000 / 1500: loss 6.274323\n",
      "iteration 1100 / 1500: loss 6.430451\n",
      "iteration 1200 / 1500: loss 6.543903\n",
      "iteration 1300 / 1500: loss 6.261545\n",
      "iteration 1400 / 1500: loss 6.918850\n",
      "training accuracy: 0.106268\n",
      "validation accuracy: 0.101020\n",
      "Got 1485 / 14700 correct => valid accuracy: 0.101020\n",
      "iteration 0 / 1500: loss 5.860651\n",
      "iteration 100 / 1500: loss 5.674755\n",
      "iteration 200 / 1500: loss 6.166385\n",
      "iteration 300 / 1500: loss 5.535535\n",
      "iteration 400 / 1500: loss 5.529497\n",
      "iteration 500 / 1500: loss 5.627789\n",
      "iteration 600 / 1500: loss 5.659941\n",
      "iteration 700 / 1500: loss 5.787752\n",
      "iteration 800 / 1500: loss 5.399690\n",
      "iteration 900 / 1500: loss 5.959738\n",
      "iteration 1000 / 1500: loss 5.675738\n",
      "iteration 1100 / 1500: loss 5.861243\n",
      "iteration 1200 / 1500: loss 5.724993\n",
      "iteration 1300 / 1500: loss 5.684312\n",
      "iteration 1400 / 1500: loss 5.855033\n",
      "training accuracy: 0.087085\n",
      "validation accuracy: 0.088231\n",
      "Got 1297 / 14700 correct => valid accuracy: 0.088231\n",
      "iteration 0 / 1500: loss 5.915122\n",
      "iteration 100 / 1500: loss 5.590748\n",
      "iteration 200 / 1500: loss 5.673020\n",
      "iteration 300 / 1500: loss 5.799087\n",
      "iteration 400 / 1500: loss 5.434134\n",
      "iteration 500 / 1500: loss 6.032104\n",
      "iteration 600 / 1500: loss 5.677302\n",
      "iteration 700 / 1500: loss 5.601496\n",
      "iteration 800 / 1500: loss 5.509907\n",
      "iteration 900 / 1500: loss 6.065888\n",
      "iteration 1000 / 1500: loss 5.571825\n",
      "iteration 1100 / 1500: loss 6.131887\n",
      "iteration 1200 / 1500: loss 5.220741\n",
      "iteration 1300 / 1500: loss 6.050428\n",
      "iteration 1400 / 1500: loss 5.720259\n",
      "training accuracy: 0.113499\n",
      "validation accuracy: 0.117551\n",
      "Got 1728 / 14700 correct => valid accuracy: 0.117551\n",
      "iteration 0 / 1500: loss 6.441578\n",
      "iteration 100 / 1500: loss 6.199347\n",
      "iteration 200 / 1500: loss 6.028738\n",
      "iteration 300 / 1500: loss 6.403215\n",
      "iteration 400 / 1500: loss 6.997317\n",
      "iteration 500 / 1500: loss 6.530893\n",
      "iteration 600 / 1500: loss 6.126482\n",
      "iteration 700 / 1500: loss 6.259720\n",
      "iteration 800 / 1500: loss 6.814432\n",
      "iteration 900 / 1500: loss 6.316491\n",
      "iteration 1000 / 1500: loss 6.276553\n",
      "iteration 1100 / 1500: loss 6.313612\n",
      "iteration 1200 / 1500: loss 6.258389\n",
      "iteration 1300 / 1500: loss 6.027157\n",
      "iteration 1400 / 1500: loss 6.168470\n",
      "training accuracy: 0.101224\n",
      "validation accuracy: 0.102993\n",
      "Got 1514 / 14700 correct => valid accuracy: 0.102993\n",
      "iteration 0 / 1500: loss 5.272003\n",
      "iteration 100 / 1500: loss 5.112115\n",
      "iteration 200 / 1500: loss 5.279978\n",
      "iteration 300 / 1500: loss 5.845038\n",
      "iteration 400 / 1500: loss 5.478465\n",
      "iteration 500 / 1500: loss 5.498755\n",
      "iteration 600 / 1500: loss 5.784246\n",
      "iteration 700 / 1500: loss 5.558698\n",
      "iteration 800 / 1500: loss 5.672961\n",
      "iteration 900 / 1500: loss 5.294690\n",
      "iteration 1000 / 1500: loss 5.417507\n",
      "iteration 1100 / 1500: loss 5.668691\n",
      "iteration 1200 / 1500: loss 5.453414\n",
      "iteration 1300 / 1500: loss 5.655928\n",
      "iteration 1400 / 1500: loss 5.623668\n",
      "training accuracy: 0.105802\n",
      "validation accuracy: 0.107891\n",
      "Got 1586 / 14700 correct => valid accuracy: 0.107891\n",
      "iteration 0 / 1500: loss 6.432207\n",
      "iteration 100 / 1500: loss 5.697777\n",
      "iteration 200 / 1500: loss 6.184918\n",
      "iteration 300 / 1500: loss 6.215117\n",
      "iteration 400 / 1500: loss 6.176415\n",
      "iteration 500 / 1500: loss 6.280674\n",
      "iteration 600 / 1500: loss 6.572307\n",
      "iteration 700 / 1500: loss 6.506188\n",
      "iteration 800 / 1500: loss 5.985610\n",
      "iteration 900 / 1500: loss 6.433593\n",
      "iteration 1000 / 1500: loss 6.263027\n",
      "iteration 1100 / 1500: loss 6.206221\n",
      "iteration 1200 / 1500: loss 6.269689\n",
      "iteration 1300 / 1500: loss 6.249805\n",
      "iteration 1400 / 1500: loss 6.186708\n",
      "training accuracy: 0.078950\n",
      "validation accuracy: 0.080476\n",
      "Got 1183 / 14700 correct => valid accuracy: 0.080476\n",
      "iteration 0 / 1500: loss 6.963144\n",
      "iteration 100 / 1500: loss 6.751100\n",
      "iteration 200 / 1500: loss 6.860557\n",
      "iteration 300 / 1500: loss 6.682668\n",
      "iteration 400 / 1500: loss 6.700147\n",
      "iteration 500 / 1500: loss 7.234241\n",
      "iteration 600 / 1500: loss 6.557489\n",
      "iteration 700 / 1500: loss 6.796797\n",
      "iteration 800 / 1500: loss 6.295404\n",
      "iteration 900 / 1500: loss 6.059356\n",
      "iteration 1000 / 1500: loss 6.769578\n",
      "iteration 1100 / 1500: loss 7.026514\n",
      "iteration 1200 / 1500: loss 6.739860\n",
      "iteration 1300 / 1500: loss 6.354665\n",
      "iteration 1400 / 1500: loss 6.745350\n",
      "training accuracy: 0.079038\n",
      "validation accuracy: 0.079048\n",
      "Got 1162 / 14700 correct => valid accuracy: 0.079048\n",
      "iteration 0 / 1500: loss 6.473043\n",
      "iteration 100 / 1500: loss 6.261050\n",
      "iteration 200 / 1500: loss 6.331132\n",
      "iteration 300 / 1500: loss 6.715243\n",
      "iteration 400 / 1500: loss 6.498880\n",
      "iteration 500 / 1500: loss 6.170370\n",
      "iteration 600 / 1500: loss 6.425233\n",
      "iteration 700 / 1500: loss 6.443564\n",
      "iteration 800 / 1500: loss 6.821720\n",
      "iteration 900 / 1500: loss 6.643050\n",
      "iteration 1000 / 1500: loss 5.888489\n",
      "iteration 1100 / 1500: loss 6.284424\n",
      "iteration 1200 / 1500: loss 6.137816\n",
      "iteration 1300 / 1500: loss 6.761523\n",
      "iteration 1400 / 1500: loss 6.519373\n",
      "training accuracy: 0.100583\n",
      "validation accuracy: 0.100204\n",
      "Got 1473 / 14700 correct => valid accuracy: 0.100204\n",
      "iteration 0 / 1500: loss 6.770974\n",
      "iteration 100 / 1500: loss 6.021170\n",
      "iteration 200 / 1500: loss 6.323516\n",
      "iteration 300 / 1500: loss 6.117994\n",
      "iteration 400 / 1500: loss 6.889805\n",
      "iteration 500 / 1500: loss 6.465965\n",
      "iteration 600 / 1500: loss 6.848088\n",
      "iteration 700 / 1500: loss 6.689990\n",
      "iteration 800 / 1500: loss 6.280858\n",
      "iteration 900 / 1500: loss 6.938719\n",
      "iteration 1000 / 1500: loss 7.062397\n",
      "iteration 1100 / 1500: loss 6.233405\n",
      "iteration 1200 / 1500: loss 6.434756\n",
      "iteration 1300 / 1500: loss 7.103789\n",
      "iteration 1400 / 1500: loss 6.041664\n",
      "training accuracy: 0.094169\n",
      "validation accuracy: 0.087891\n",
      "Got 1292 / 14700 correct => valid accuracy: 0.087891\n",
      "iteration 0 / 1500: loss 5.643553\n",
      "iteration 100 / 1500: loss 5.953410\n",
      "iteration 200 / 1500: loss 5.748439\n",
      "iteration 300 / 1500: loss 5.986993\n",
      "iteration 400 / 1500: loss 5.991227\n",
      "iteration 500 / 1500: loss 6.347945\n",
      "iteration 600 / 1500: loss 5.777175\n",
      "iteration 700 / 1500: loss 5.971012\n",
      "iteration 800 / 1500: loss 5.867688\n",
      "iteration 900 / 1500: loss 5.934135\n",
      "iteration 1000 / 1500: loss 5.956613\n",
      "iteration 1100 / 1500: loss 5.484132\n",
      "iteration 1200 / 1500: loss 5.798393\n",
      "iteration 1300 / 1500: loss 5.921682\n",
      "iteration 1400 / 1500: loss 5.547385\n",
      "training accuracy: 0.102128\n",
      "validation accuracy: 0.101565\n",
      "Got 1493 / 14700 correct => valid accuracy: 0.101565\n",
      "iteration 0 / 1500: loss 5.878656\n",
      "iteration 100 / 1500: loss 5.697256\n",
      "iteration 200 / 1500: loss 5.995899\n",
      "iteration 300 / 1500: loss 5.833287\n",
      "iteration 400 / 1500: loss 6.249622\n",
      "iteration 500 / 1500: loss 6.003852\n",
      "iteration 600 / 1500: loss 5.469488\n",
      "iteration 700 / 1500: loss 5.674622\n",
      "iteration 800 / 1500: loss 5.917759\n",
      "iteration 900 / 1500: loss 5.714894\n",
      "iteration 1000 / 1500: loss 5.974958\n",
      "iteration 1100 / 1500: loss 5.991008\n",
      "iteration 1200 / 1500: loss 6.276984\n",
      "iteration 1300 / 1500: loss 6.312849\n",
      "iteration 1400 / 1500: loss 5.811197\n",
      "training accuracy: 0.083673\n",
      "validation accuracy: 0.084218\n",
      "Got 1238 / 14700 correct => valid accuracy: 0.084218\n",
      "iteration 0 / 1500: loss 5.406872\n",
      "iteration 100 / 1500: loss 6.091823\n",
      "iteration 200 / 1500: loss 6.162271\n",
      "iteration 300 / 1500: loss 6.390255\n",
      "iteration 400 / 1500: loss 6.018207\n",
      "iteration 500 / 1500: loss 6.134983\n",
      "iteration 600 / 1500: loss 6.107755\n",
      "iteration 700 / 1500: loss 5.605436\n",
      "iteration 800 / 1500: loss 6.134259\n",
      "iteration 900 / 1500: loss 6.091954\n",
      "iteration 1000 / 1500: loss 6.346013\n",
      "iteration 1100 / 1500: loss 6.134341\n",
      "iteration 1200 / 1500: loss 6.426312\n",
      "iteration 1300 / 1500: loss 6.161067\n",
      "iteration 1400 / 1500: loss 5.665684\n",
      "training accuracy: 0.089475\n",
      "validation accuracy: 0.087211\n",
      "Got 1282 / 14700 correct => valid accuracy: 0.087211\n",
      "iteration 0 / 1500: loss 5.345486\n",
      "iteration 100 / 1500: loss 5.789056\n",
      "iteration 200 / 1500: loss 5.169315\n",
      "iteration 300 / 1500: loss 5.346657\n",
      "iteration 400 / 1500: loss 5.232121\n",
      "iteration 500 / 1500: loss 5.551939\n",
      "iteration 600 / 1500: loss 5.430876\n",
      "iteration 700 / 1500: loss 5.515077\n",
      "iteration 800 / 1500: loss 5.499126\n",
      "iteration 900 / 1500: loss 5.739987\n",
      "iteration 1000 / 1500: loss 5.297103\n",
      "iteration 1100 / 1500: loss 5.338641\n",
      "iteration 1200 / 1500: loss 5.465452\n",
      "iteration 1300 / 1500: loss 5.541268\n",
      "iteration 1400 / 1500: loss 6.407106\n",
      "training accuracy: 0.094840\n",
      "validation accuracy: 0.093810\n",
      "Got 1379 / 14700 correct => valid accuracy: 0.093810\n",
      "iteration 0 / 1500: loss 5.888239\n",
      "iteration 100 / 1500: loss 5.341332\n",
      "iteration 200 / 1500: loss 5.761980\n",
      "iteration 300 / 1500: loss 5.754369\n",
      "iteration 400 / 1500: loss 5.547315\n",
      "iteration 500 / 1500: loss 5.874930\n",
      "iteration 600 / 1500: loss 5.210870\n",
      "iteration 700 / 1500: loss 5.651312\n",
      "iteration 800 / 1500: loss 5.872565\n",
      "iteration 900 / 1500: loss 5.985107\n",
      "iteration 1000 / 1500: loss 5.664861\n",
      "iteration 1100 / 1500: loss 5.876341\n",
      "iteration 1200 / 1500: loss 5.607430\n",
      "iteration 1300 / 1500: loss 6.058845\n",
      "iteration 1400 / 1500: loss 5.954695\n",
      "training accuracy: 0.111341\n",
      "validation accuracy: 0.114830\n",
      "Got 1688 / 14700 correct => valid accuracy: 0.114830\n",
      "iteration 0 / 1500: loss 6.463624\n",
      "iteration 100 / 1500: loss 6.678977\n",
      "iteration 200 / 1500: loss 6.302542\n",
      "iteration 300 / 1500: loss 6.548389\n",
      "iteration 400 / 1500: loss 6.460604\n",
      "iteration 500 / 1500: loss 7.345144\n",
      "iteration 600 / 1500: loss 6.738991\n",
      "iteration 700 / 1500: loss 6.420272\n",
      "iteration 800 / 1500: loss 6.160082\n",
      "iteration 900 / 1500: loss 6.386338\n",
      "iteration 1000 / 1500: loss 6.626523\n",
      "iteration 1100 / 1500: loss 6.948408\n",
      "iteration 1200 / 1500: loss 6.140730\n",
      "iteration 1300 / 1500: loss 6.890020\n",
      "iteration 1400 / 1500: loss 6.071162\n",
      "training accuracy: 0.102245\n",
      "validation accuracy: 0.102993\n",
      "Got 1514 / 14700 correct => valid accuracy: 0.102993\n",
      "iteration 0 / 1500: loss 5.964395\n",
      "iteration 100 / 1500: loss 5.774043\n",
      "iteration 200 / 1500: loss 6.050252\n",
      "iteration 300 / 1500: loss 5.627485\n",
      "iteration 400 / 1500: loss 5.500096\n",
      "iteration 500 / 1500: loss 5.944577\n",
      "iteration 600 / 1500: loss 5.594841\n",
      "iteration 700 / 1500: loss 5.611548\n",
      "iteration 800 / 1500: loss 5.775907\n",
      "iteration 900 / 1500: loss 5.821281\n",
      "iteration 1000 / 1500: loss 5.514833\n",
      "iteration 1100 / 1500: loss 5.682556\n",
      "iteration 1200 / 1500: loss 5.390541\n",
      "iteration 1300 / 1500: loss 5.662154\n",
      "iteration 1400 / 1500: loss 5.668891\n",
      "training accuracy: 0.109359\n",
      "validation accuracy: 0.108231\n",
      "Got 1591 / 14700 correct => valid accuracy: 0.108231\n",
      "iteration 0 / 1500: loss 6.654062\n",
      "iteration 100 / 1500: loss 6.809499\n",
      "iteration 200 / 1500: loss 7.034146\n",
      "iteration 300 / 1500: loss 7.310613\n",
      "iteration 400 / 1500: loss 6.560034\n",
      "iteration 500 / 1500: loss 7.143330\n",
      "iteration 600 / 1500: loss 6.853128\n",
      "iteration 700 / 1500: loss 6.830725\n",
      "iteration 800 / 1500: loss 6.781686\n",
      "iteration 900 / 1500: loss 6.742339\n",
      "iteration 1000 / 1500: loss 6.456610\n",
      "iteration 1100 / 1500: loss 6.795395\n",
      "iteration 1200 / 1500: loss 6.976022\n",
      "iteration 1300 / 1500: loss 6.596164\n",
      "iteration 1400 / 1500: loss 6.604596\n",
      "training accuracy: 0.106560\n",
      "validation accuracy: 0.104830\n",
      "Got 1541 / 14700 correct => valid accuracy: 0.104830\n",
      "iteration 0 / 1500: loss 4.874916\n",
      "iteration 100 / 1500: loss 5.338000\n",
      "iteration 200 / 1500: loss 4.802247\n",
      "iteration 300 / 1500: loss 4.709374\n",
      "iteration 400 / 1500: loss 4.903831\n",
      "iteration 500 / 1500: loss 5.159918\n",
      "iteration 600 / 1500: loss 4.747769\n",
      "iteration 700 / 1500: loss 5.117359\n",
      "iteration 800 / 1500: loss 4.985977\n",
      "iteration 900 / 1500: loss 5.020466\n",
      "iteration 1000 / 1500: loss 5.232464\n",
      "iteration 1100 / 1500: loss 5.020300\n",
      "iteration 1200 / 1500: loss 4.948073\n",
      "iteration 1300 / 1500: loss 4.653499\n",
      "iteration 1400 / 1500: loss 4.562489\n",
      "training accuracy: 0.124548\n",
      "validation accuracy: 0.129932\n",
      "Got 1910 / 14700 correct => valid accuracy: 0.129932\n",
      "iteration 0 / 1500: loss 6.914454\n",
      "iteration 100 / 1500: loss 5.646711\n",
      "iteration 200 / 1500: loss 6.249993\n",
      "iteration 300 / 1500: loss 6.661244\n",
      "iteration 400 / 1500: loss 7.325121\n",
      "iteration 500 / 1500: loss 6.990112\n",
      "iteration 600 / 1500: loss 6.813134\n",
      "iteration 700 / 1500: loss 6.421500\n",
      "iteration 800 / 1500: loss 6.193780\n",
      "iteration 900 / 1500: loss 6.689267\n",
      "iteration 1000 / 1500: loss 6.958862\n",
      "iteration 1100 / 1500: loss 6.457405\n",
      "iteration 1200 / 1500: loss 6.463011\n",
      "iteration 1300 / 1500: loss 6.486936\n",
      "iteration 1400 / 1500: loss 6.730920\n",
      "training accuracy: 0.074606\n",
      "validation accuracy: 0.077143\n",
      "Got 1134 / 14700 correct => valid accuracy: 0.077143\n",
      "iteration 0 / 1500: loss 6.257220\n",
      "iteration 100 / 1500: loss 7.094911\n",
      "iteration 200 / 1500: loss 6.817508\n",
      "iteration 300 / 1500: loss 7.363686\n",
      "iteration 400 / 1500: loss 6.766321\n",
      "iteration 500 / 1500: loss 6.616540\n",
      "iteration 600 / 1500: loss 7.017036\n",
      "iteration 700 / 1500: loss 6.766326\n",
      "iteration 800 / 1500: loss 6.334277\n",
      "iteration 900 / 1500: loss 7.329254\n",
      "iteration 1000 / 1500: loss 6.955634\n",
      "iteration 1100 / 1500: loss 7.136698\n",
      "iteration 1200 / 1500: loss 6.981397\n",
      "iteration 1300 / 1500: loss 6.799518\n",
      "iteration 1400 / 1500: loss 6.219680\n",
      "training accuracy: 0.086152\n",
      "validation accuracy: 0.083401\n",
      "Got 1226 / 14700 correct => valid accuracy: 0.083401\n",
      "iteration 0 / 1500: loss 5.974348\n",
      "iteration 100 / 1500: loss 5.995162\n",
      "iteration 200 / 1500: loss 6.130723\n",
      "iteration 300 / 1500: loss 6.055080\n",
      "iteration 400 / 1500: loss 5.991957\n",
      "iteration 500 / 1500: loss 5.689147\n",
      "iteration 600 / 1500: loss 6.739727\n",
      "iteration 700 / 1500: loss 6.011064\n",
      "iteration 800 / 1500: loss 6.414341\n",
      "iteration 900 / 1500: loss 5.758051\n",
      "iteration 1000 / 1500: loss 6.320429\n",
      "iteration 1100 / 1500: loss 6.119222\n",
      "iteration 1200 / 1500: loss 5.789943\n",
      "iteration 1300 / 1500: loss 5.912780\n",
      "iteration 1400 / 1500: loss 6.294351\n",
      "training accuracy: 0.081924\n",
      "validation accuracy: 0.088367\n",
      "Got 1299 / 14700 correct => valid accuracy: 0.088367\n",
      "iteration 0 / 1500: loss 6.091639\n",
      "iteration 100 / 1500: loss 6.436319\n",
      "iteration 200 / 1500: loss 6.079665\n",
      "iteration 300 / 1500: loss 6.243138\n",
      "iteration 400 / 1500: loss 6.105170\n",
      "iteration 500 / 1500: loss 5.715035\n",
      "iteration 600 / 1500: loss 5.996867\n",
      "iteration 700 / 1500: loss 6.220008\n",
      "iteration 800 / 1500: loss 6.306175\n",
      "iteration 900 / 1500: loss 6.233774\n",
      "iteration 1000 / 1500: loss 6.313550\n",
      "iteration 1100 / 1500: loss 6.745521\n",
      "iteration 1200 / 1500: loss 6.244258\n",
      "iteration 1300 / 1500: loss 6.293830\n",
      "iteration 1400 / 1500: loss 6.277506\n",
      "training accuracy: 0.086880\n",
      "validation accuracy: 0.087211\n",
      "Got 1282 / 14700 correct => valid accuracy: 0.087211\n",
      "iteration 0 / 1500: loss 5.322333\n",
      "iteration 100 / 1500: loss 5.402846\n",
      "iteration 200 / 1500: loss 5.410316\n",
      "iteration 300 / 1500: loss 5.218983\n",
      "iteration 400 / 1500: loss 5.392148\n",
      "iteration 500 / 1500: loss 5.262464\n",
      "iteration 600 / 1500: loss 5.448889\n",
      "iteration 700 / 1500: loss 6.043306\n",
      "iteration 800 / 1500: loss 5.690736\n",
      "iteration 900 / 1500: loss 5.394942\n",
      "iteration 1000 / 1500: loss 5.406953\n",
      "iteration 1100 / 1500: loss 5.273858\n",
      "iteration 1200 / 1500: loss 5.357766\n",
      "iteration 1300 / 1500: loss 5.207216\n",
      "iteration 1400 / 1500: loss 4.969654\n",
      "training accuracy: 0.121429\n",
      "validation accuracy: 0.126735\n",
      "Got 1863 / 14700 correct => valid accuracy: 0.126735\n",
      "iteration 0 / 1500: loss 6.427554\n",
      "iteration 100 / 1500: loss 6.047252\n",
      "iteration 200 / 1500: loss 6.090518\n",
      "iteration 300 / 1500: loss 6.246940\n",
      "iteration 400 / 1500: loss 6.174991\n",
      "iteration 500 / 1500: loss 6.363485\n",
      "iteration 600 / 1500: loss 6.339534\n",
      "iteration 700 / 1500: loss 6.073956\n",
      "iteration 800 / 1500: loss 5.714210\n",
      "iteration 900 / 1500: loss 6.291162\n",
      "iteration 1000 / 1500: loss 6.271574\n",
      "iteration 1100 / 1500: loss 6.130791\n",
      "iteration 1200 / 1500: loss 6.052930\n",
      "iteration 1300 / 1500: loss 6.229251\n",
      "iteration 1400 / 1500: loss 5.384938\n",
      "training accuracy: 0.087085\n",
      "validation accuracy: 0.086327\n",
      "Got 1269 / 14700 correct => valid accuracy: 0.086327\n",
      "iteration 0 / 1500: loss 7.820186\n",
      "iteration 100 / 1500: loss 7.428110\n",
      "iteration 200 / 1500: loss 7.577599\n",
      "iteration 300 / 1500: loss 7.772707\n",
      "iteration 400 / 1500: loss 7.127741\n",
      "iteration 500 / 1500: loss 7.553703\n",
      "iteration 600 / 1500: loss 7.378452\n",
      "iteration 700 / 1500: loss 7.217555\n",
      "iteration 800 / 1500: loss 7.349948\n",
      "iteration 900 / 1500: loss 7.547363\n",
      "iteration 1000 / 1500: loss 7.391594\n",
      "iteration 1100 / 1500: loss 7.758276\n",
      "iteration 1200 / 1500: loss 7.561178\n",
      "iteration 1300 / 1500: loss 7.345431\n",
      "iteration 1400 / 1500: loss 7.212566\n",
      "training accuracy: 0.091050\n",
      "validation accuracy: 0.087007\n",
      "Got 1279 / 14700 correct => valid accuracy: 0.087007\n",
      "iteration 0 / 1500: loss 5.922296\n",
      "iteration 100 / 1500: loss 6.051589\n",
      "iteration 200 / 1500: loss 6.471885\n",
      "iteration 300 / 1500: loss 6.254495\n",
      "iteration 400 / 1500: loss 6.610495\n",
      "iteration 500 / 1500: loss 6.565109\n",
      "iteration 600 / 1500: loss 6.684151\n",
      "iteration 700 / 1500: loss 6.744397\n",
      "iteration 800 / 1500: loss 6.997100\n",
      "iteration 900 / 1500: loss 6.304713\n",
      "iteration 1000 / 1500: loss 6.317664\n",
      "iteration 1100 / 1500: loss 6.040179\n",
      "iteration 1200 / 1500: loss 6.034223\n",
      "iteration 1300 / 1500: loss 6.358367\n",
      "iteration 1400 / 1500: loss 6.506875\n",
      "training accuracy: 0.080233\n",
      "validation accuracy: 0.079048\n",
      "Got 1162 / 14700 correct => valid accuracy: 0.079048\n",
      "iteration 0 / 1500: loss 6.628751\n",
      "iteration 100 / 1500: loss 5.827369\n",
      "iteration 200 / 1500: loss 6.718091\n",
      "iteration 300 / 1500: loss 6.414570\n",
      "iteration 400 / 1500: loss 5.943697\n",
      "iteration 500 / 1500: loss 6.186989\n",
      "iteration 600 / 1500: loss 6.210722\n",
      "iteration 700 / 1500: loss 6.270636\n",
      "iteration 800 / 1500: loss 6.430340\n",
      "iteration 900 / 1500: loss 6.165483\n",
      "iteration 1000 / 1500: loss 6.446608\n",
      "iteration 1100 / 1500: loss 6.057261\n",
      "iteration 1200 / 1500: loss 6.555269\n",
      "iteration 1300 / 1500: loss 6.501052\n",
      "iteration 1400 / 1500: loss 5.850210\n",
      "training accuracy: 0.110029\n",
      "validation accuracy: 0.107483\n",
      "Got 1580 / 14700 correct => valid accuracy: 0.107483\n",
      "iteration 0 / 1500: loss 6.231866\n",
      "iteration 100 / 1500: loss 5.907321\n",
      "iteration 200 / 1500: loss 6.127585\n",
      "iteration 300 / 1500: loss 5.941138\n",
      "iteration 400 / 1500: loss 5.824190\n",
      "iteration 500 / 1500: loss 6.026704\n",
      "iteration 600 / 1500: loss 6.235333\n",
      "iteration 700 / 1500: loss 6.205635\n",
      "iteration 800 / 1500: loss 5.470190\n",
      "iteration 900 / 1500: loss 5.659512\n",
      "iteration 1000 / 1500: loss 6.023361\n",
      "iteration 1100 / 1500: loss 5.321917\n",
      "iteration 1200 / 1500: loss 5.852717\n",
      "iteration 1300 / 1500: loss 5.510766\n",
      "iteration 1400 / 1500: loss 5.783773\n",
      "training accuracy: 0.094023\n",
      "validation accuracy: 0.090000\n",
      "Got 1323 / 14700 correct => valid accuracy: 0.090000\n",
      "iteration 0 / 1500: loss 6.138262\n",
      "iteration 100 / 1500: loss 5.891691\n",
      "iteration 200 / 1500: loss 6.055405\n",
      "iteration 300 / 1500: loss 6.022445\n",
      "iteration 400 / 1500: loss 6.393945\n",
      "iteration 500 / 1500: loss 6.337225\n",
      "iteration 600 / 1500: loss 6.020590\n",
      "iteration 700 / 1500: loss 6.070787\n",
      "iteration 800 / 1500: loss 5.596240\n",
      "iteration 900 / 1500: loss 6.201099\n",
      "iteration 1000 / 1500: loss 6.088073\n",
      "iteration 1100 / 1500: loss 5.471575\n",
      "iteration 1200 / 1500: loss 6.277975\n",
      "iteration 1300 / 1500: loss 6.032293\n",
      "iteration 1400 / 1500: loss 6.274923\n",
      "training accuracy: 0.081691\n",
      "validation accuracy: 0.079796\n",
      "Got 1173 / 14700 correct => valid accuracy: 0.079796\n",
      "iteration 0 / 1500: loss 5.430137\n",
      "iteration 100 / 1500: loss 5.232503\n",
      "iteration 200 / 1500: loss 5.473479\n",
      "iteration 300 / 1500: loss 5.671294\n",
      "iteration 400 / 1500: loss 5.531148\n",
      "iteration 500 / 1500: loss 5.632769\n",
      "iteration 600 / 1500: loss 5.560022\n",
      "iteration 700 / 1500: loss 5.435900\n",
      "iteration 800 / 1500: loss 5.612070\n",
      "iteration 900 / 1500: loss 5.293534\n",
      "iteration 1000 / 1500: loss 5.725484\n",
      "iteration 1100 / 1500: loss 5.783481\n",
      "iteration 1200 / 1500: loss 4.853853\n",
      "iteration 1300 / 1500: loss 5.373044\n",
      "iteration 1400 / 1500: loss 5.535870\n",
      "training accuracy: 0.110700\n",
      "validation accuracy: 0.112857\n",
      "Got 1659 / 14700 correct => valid accuracy: 0.112857\n",
      "iteration 0 / 1500: loss 6.780673\n",
      "iteration 100 / 1500: loss 6.196340\n",
      "iteration 200 / 1500: loss 6.705479\n",
      "iteration 300 / 1500: loss 6.695507\n",
      "iteration 400 / 1500: loss 6.441054\n",
      "iteration 500 / 1500: loss 6.606248\n",
      "iteration 600 / 1500: loss 6.862895\n",
      "iteration 700 / 1500: loss 7.280859\n",
      "iteration 800 / 1500: loss 6.854443\n",
      "iteration 900 / 1500: loss 6.547665\n",
      "iteration 1000 / 1500: loss 6.794927\n",
      "iteration 1100 / 1500: loss 6.545516\n",
      "iteration 1200 / 1500: loss 6.176066\n",
      "iteration 1300 / 1500: loss 6.555745\n",
      "iteration 1400 / 1500: loss 6.396964\n",
      "training accuracy: 0.084373\n",
      "validation accuracy: 0.084014\n",
      "Got 1235 / 14700 correct => valid accuracy: 0.084014\n",
      "iteration 0 / 1500: loss 7.308982\n",
      "iteration 100 / 1500: loss 7.192209\n",
      "iteration 200 / 1500: loss 6.944237\n",
      "iteration 300 / 1500: loss 7.105289\n",
      "iteration 400 / 1500: loss 6.917562\n",
      "iteration 500 / 1500: loss 7.200887\n",
      "iteration 600 / 1500: loss 6.586305\n",
      "iteration 700 / 1500: loss 7.144446\n",
      "iteration 800 / 1500: loss 7.568166\n",
      "iteration 900 / 1500: loss 7.607948\n",
      "iteration 1000 / 1500: loss 7.576122\n",
      "iteration 1100 / 1500: loss 7.227918\n",
      "iteration 1200 / 1500: loss 7.385595\n",
      "iteration 1300 / 1500: loss 7.146129\n",
      "iteration 1400 / 1500: loss 6.753648\n",
      "training accuracy: 0.082332\n",
      "validation accuracy: 0.079048\n",
      "Got 1162 / 14700 correct => valid accuracy: 0.079048\n",
      "iteration 0 / 1500: loss 7.108708\n",
      "iteration 100 / 1500: loss 6.847722\n",
      "iteration 200 / 1500: loss 7.306090\n",
      "iteration 300 / 1500: loss 7.130549\n",
      "iteration 400 / 1500: loss 7.461169\n",
      "iteration 500 / 1500: loss 7.225989\n",
      "iteration 600 / 1500: loss 7.027862\n",
      "iteration 700 / 1500: loss 7.135955\n",
      "iteration 800 / 1500: loss 7.840593\n",
      "iteration 900 / 1500: loss 7.275514\n",
      "iteration 1000 / 1500: loss 7.146092\n",
      "iteration 1100 / 1500: loss 7.756031\n",
      "iteration 1200 / 1500: loss 7.209283\n",
      "iteration 1300 / 1500: loss 7.237300\n",
      "iteration 1400 / 1500: loss 7.519155\n",
      "training accuracy: 0.086443\n",
      "validation accuracy: 0.083537\n",
      "Got 1228 / 14700 correct => valid accuracy: 0.083537\n",
      "iteration 0 / 1500: loss 6.590616\n",
      "iteration 100 / 1500: loss 6.239526\n",
      "iteration 200 / 1500: loss 5.884077\n",
      "iteration 300 / 1500: loss 6.098328\n",
      "iteration 400 / 1500: loss 6.165180\n",
      "iteration 500 / 1500: loss 6.646460\n",
      "iteration 600 / 1500: loss 5.930085\n",
      "iteration 700 / 1500: loss 6.185920\n",
      "iteration 800 / 1500: loss 6.094474\n",
      "iteration 900 / 1500: loss 6.334039\n",
      "iteration 1000 / 1500: loss 6.072629\n",
      "iteration 1100 / 1500: loss 5.874168\n",
      "iteration 1200 / 1500: loss 6.349329\n",
      "iteration 1300 / 1500: loss 6.155293\n",
      "iteration 1400 / 1500: loss 6.220644\n",
      "training accuracy: 0.067580\n",
      "validation accuracy: 0.074626\n",
      "Got 1097 / 14700 correct => valid accuracy: 0.074626\n",
      "iteration 0 / 1500: loss 4.881619\n",
      "iteration 100 / 1500: loss 4.978230\n",
      "iteration 200 / 1500: loss 4.933092\n",
      "iteration 300 / 1500: loss 4.765373\n",
      "iteration 400 / 1500: loss 4.285335\n",
      "iteration 500 / 1500: loss 4.799455\n",
      "iteration 600 / 1500: loss 4.587213\n",
      "iteration 700 / 1500: loss 4.743967\n",
      "iteration 800 / 1500: loss 4.489620\n",
      "iteration 900 / 1500: loss 4.623329\n",
      "iteration 1000 / 1500: loss 4.500440\n",
      "iteration 1100 / 1500: loss 4.251704\n",
      "iteration 1200 / 1500: loss 4.404576\n",
      "iteration 1300 / 1500: loss 5.063488\n",
      "iteration 1400 / 1500: loss 4.555208\n",
      "training accuracy: 0.125977\n",
      "validation accuracy: 0.121837\n",
      "Got 1791 / 14700 correct => valid accuracy: 0.121837\n",
      "iteration 0 / 1500: loss 5.495609\n",
      "iteration 100 / 1500: loss 5.486863\n",
      "iteration 200 / 1500: loss 5.402588\n",
      "iteration 300 / 1500: loss 5.559735\n",
      "iteration 400 / 1500: loss 5.669559\n",
      "iteration 500 / 1500: loss 5.224870\n",
      "iteration 600 / 1500: loss 5.426524\n",
      "iteration 700 / 1500: loss 5.741087\n",
      "iteration 800 / 1500: loss 5.566133\n",
      "iteration 900 / 1500: loss 5.532332\n",
      "iteration 1000 / 1500: loss 5.249469\n",
      "iteration 1100 / 1500: loss 5.916530\n",
      "iteration 1200 / 1500: loss 5.212697\n",
      "iteration 1300 / 1500: loss 5.693550\n",
      "iteration 1400 / 1500: loss 5.999518\n",
      "training accuracy: 0.097784\n",
      "validation accuracy: 0.100544\n",
      "Got 1478 / 14700 correct => valid accuracy: 0.100544\n",
      "iteration 0 / 1500: loss 6.104640\n",
      "iteration 100 / 1500: loss 5.869914\n",
      "iteration 200 / 1500: loss 6.127645\n",
      "iteration 300 / 1500: loss 6.431504\n",
      "iteration 400 / 1500: loss 6.067062\n",
      "iteration 500 / 1500: loss 6.523385\n",
      "iteration 600 / 1500: loss 6.120014\n",
      "iteration 700 / 1500: loss 6.510743\n",
      "iteration 800 / 1500: loss 6.231030\n",
      "iteration 900 / 1500: loss 5.993575\n",
      "iteration 1000 / 1500: loss 6.300433\n",
      "iteration 1100 / 1500: loss 6.285377\n",
      "iteration 1200 / 1500: loss 5.683726\n",
      "iteration 1300 / 1500: loss 6.198829\n",
      "iteration 1400 / 1500: loss 6.006381\n",
      "training accuracy: 0.093265\n",
      "validation accuracy: 0.092585\n",
      "Got 1361 / 14700 correct => valid accuracy: 0.092585\n",
      "iteration 0 / 1500: loss 5.845224\n",
      "iteration 100 / 1500: loss 5.760684\n",
      "iteration 200 / 1500: loss 6.104454\n",
      "iteration 300 / 1500: loss 5.834788\n",
      "iteration 400 / 1500: loss 6.024196\n",
      "iteration 500 / 1500: loss 5.791481\n",
      "iteration 600 / 1500: loss 6.120635\n",
      "iteration 700 / 1500: loss 6.076876\n",
      "iteration 800 / 1500: loss 5.923381\n",
      "iteration 900 / 1500: loss 5.687176\n",
      "iteration 1000 / 1500: loss 5.636006\n",
      "iteration 1100 / 1500: loss 5.911381\n",
      "iteration 1200 / 1500: loss 5.878778\n",
      "iteration 1300 / 1500: loss 5.953182\n",
      "iteration 1400 / 1500: loss 5.653051\n",
      "training accuracy: 0.090029\n",
      "validation accuracy: 0.088095\n",
      "Got 1295 / 14700 correct => valid accuracy: 0.088095\n",
      "iteration 0 / 1500: loss 6.009155\n",
      "iteration 100 / 1500: loss 5.737433\n",
      "iteration 200 / 1500: loss 6.199526\n",
      "iteration 300 / 1500: loss 5.805707\n",
      "iteration 400 / 1500: loss 5.775925\n",
      "iteration 500 / 1500: loss 6.350099\n",
      "iteration 600 / 1500: loss 6.309209\n",
      "iteration 700 / 1500: loss 5.999254\n",
      "iteration 800 / 1500: loss 5.663839\n",
      "iteration 900 / 1500: loss 5.721728\n",
      "iteration 1000 / 1500: loss 5.725791\n",
      "iteration 1100 / 1500: loss 6.203732\n",
      "iteration 1200 / 1500: loss 6.097833\n",
      "iteration 1300 / 1500: loss 5.592019\n",
      "iteration 1400 / 1500: loss 5.782091\n",
      "training accuracy: 0.089096\n",
      "validation accuracy: 0.084898\n",
      "Got 1248 / 14700 correct => valid accuracy: 0.084898\n",
      "iteration 0 / 1500: loss 7.041299\n",
      "iteration 100 / 1500: loss 7.095805\n",
      "iteration 200 / 1500: loss 7.215836\n",
      "iteration 300 / 1500: loss 6.793738\n",
      "iteration 400 / 1500: loss 8.008255\n",
      "iteration 500 / 1500: loss 7.853627\n",
      "iteration 600 / 1500: loss 7.676113\n",
      "iteration 700 / 1500: loss 7.553866\n",
      "iteration 800 / 1500: loss 6.958047\n",
      "iteration 900 / 1500: loss 7.473540\n",
      "iteration 1000 / 1500: loss 7.507667\n",
      "iteration 1100 / 1500: loss 7.225041\n",
      "iteration 1200 / 1500: loss 7.406758\n",
      "iteration 1300 / 1500: loss 7.575175\n",
      "iteration 1400 / 1500: loss 7.188635\n",
      "training accuracy: 0.090000\n",
      "validation accuracy: 0.087007\n",
      "Got 1279 / 14700 correct => valid accuracy: 0.087007\n",
      "iteration 0 / 1500: loss 7.166168\n",
      "iteration 100 / 1500: loss 6.825323\n",
      "iteration 200 / 1500: loss 6.957121\n",
      "iteration 300 / 1500: loss 6.594731\n",
      "iteration 400 / 1500: loss 6.684246\n",
      "iteration 500 / 1500: loss 6.631067\n",
      "iteration 600 / 1500: loss 6.607179\n",
      "iteration 700 / 1500: loss 6.943043\n",
      "iteration 800 / 1500: loss 6.946567\n",
      "iteration 900 / 1500: loss 7.019899\n",
      "iteration 1000 / 1500: loss 6.084760\n",
      "iteration 1100 / 1500: loss 6.495289\n",
      "iteration 1200 / 1500: loss 7.206735\n",
      "iteration 1300 / 1500: loss 6.475357\n",
      "iteration 1400 / 1500: loss 6.304496\n",
      "training accuracy: 0.106618\n",
      "validation accuracy: 0.101497\n",
      "Got 1492 / 14700 correct => valid accuracy: 0.101497\n",
      "iteration 0 / 1500: loss 6.144990\n",
      "iteration 100 / 1500: loss 5.694898\n",
      "iteration 200 / 1500: loss 6.036720\n",
      "iteration 300 / 1500: loss 5.377678\n",
      "iteration 400 / 1500: loss 5.449415\n",
      "iteration 500 / 1500: loss 6.009518\n",
      "iteration 600 / 1500: loss 5.726358\n",
      "iteration 700 / 1500: loss 6.047583\n",
      "iteration 800 / 1500: loss 5.461918\n",
      "iteration 900 / 1500: loss 6.202084\n",
      "iteration 1000 / 1500: loss 5.813956\n",
      "iteration 1100 / 1500: loss 5.793185\n",
      "iteration 1200 / 1500: loss 5.856167\n",
      "iteration 1300 / 1500: loss 5.681769\n",
      "iteration 1400 / 1500: loss 5.992412\n",
      "training accuracy: 0.092362\n",
      "validation accuracy: 0.090272\n",
      "Got 1327 / 14700 correct => valid accuracy: 0.090272\n",
      "iteration 0 / 1500: loss 5.186145\n",
      "iteration 100 / 1500: loss 5.231612\n",
      "iteration 200 / 1500: loss 5.648687\n",
      "iteration 300 / 1500: loss 5.825300\n",
      "iteration 400 / 1500: loss 5.163870\n",
      "iteration 500 / 1500: loss 5.255012\n",
      "iteration 600 / 1500: loss 5.615621\n",
      "iteration 700 / 1500: loss 5.121463\n",
      "iteration 800 / 1500: loss 5.309244\n",
      "iteration 900 / 1500: loss 5.386644\n",
      "iteration 1000 / 1500: loss 5.472045\n",
      "iteration 1100 / 1500: loss 5.528795\n",
      "iteration 1200 / 1500: loss 5.606849\n",
      "iteration 1300 / 1500: loss 5.083808\n",
      "iteration 1400 / 1500: loss 5.443402\n",
      "training accuracy: 0.107464\n",
      "validation accuracy: 0.101293\n",
      "Got 1489 / 14700 correct => valid accuracy: 0.101293\n",
      "iteration 0 / 1500: loss 4.823201\n",
      "iteration 100 / 1500: loss 4.932164\n",
      "iteration 200 / 1500: loss 4.848477\n",
      "iteration 300 / 1500: loss 4.989109\n",
      "iteration 400 / 1500: loss 5.049402\n",
      "iteration 500 / 1500: loss 4.938918\n",
      "iteration 600 / 1500: loss 4.990641\n",
      "iteration 700 / 1500: loss 4.921761\n",
      "iteration 800 / 1500: loss 5.451040\n",
      "iteration 900 / 1500: loss 5.306915\n",
      "iteration 1000 / 1500: loss 5.169821\n",
      "iteration 1100 / 1500: loss 4.727534\n",
      "iteration 1200 / 1500: loss 5.103106\n",
      "iteration 1300 / 1500: loss 5.180288\n",
      "iteration 1400 / 1500: loss 5.117588\n",
      "training accuracy: 0.117085\n",
      "validation accuracy: 0.117755\n",
      "Got 1731 / 14700 correct => valid accuracy: 0.117755\n",
      "iteration 0 / 1500: loss 6.869677\n",
      "iteration 100 / 1500: loss 6.577995\n",
      "iteration 200 / 1500: loss 6.892629\n",
      "iteration 300 / 1500: loss 7.245701\n",
      "iteration 400 / 1500: loss 6.919283\n",
      "iteration 500 / 1500: loss 6.797990\n",
      "iteration 600 / 1500: loss 6.778256\n",
      "iteration 700 / 1500: loss 7.221471\n",
      "iteration 800 / 1500: loss 6.904817\n",
      "iteration 900 / 1500: loss 7.204819\n",
      "iteration 1000 / 1500: loss 6.859885\n",
      "iteration 1100 / 1500: loss 6.834609\n",
      "iteration 1200 / 1500: loss 6.288763\n",
      "iteration 1300 / 1500: loss 7.150991\n",
      "iteration 1400 / 1500: loss 6.677679\n",
      "training accuracy: 0.089213\n",
      "validation accuracy: 0.087143\n",
      "Got 1281 / 14700 correct => valid accuracy: 0.087143\n",
      "iteration 0 / 1500: loss 6.342930\n",
      "iteration 100 / 1500: loss 5.709331\n",
      "iteration 200 / 1500: loss 6.297915\n",
      "iteration 300 / 1500: loss 6.292340\n",
      "iteration 400 / 1500: loss 6.074864\n",
      "iteration 500 / 1500: loss 5.800456\n",
      "iteration 600 / 1500: loss 6.361098\n",
      "iteration 700 / 1500: loss 6.442504\n",
      "iteration 800 / 1500: loss 6.065704\n",
      "iteration 900 / 1500: loss 5.784225\n",
      "iteration 1000 / 1500: loss 6.275786\n",
      "iteration 1100 / 1500: loss 5.757025\n",
      "iteration 1200 / 1500: loss 5.955818\n",
      "iteration 1300 / 1500: loss 6.438050\n",
      "iteration 1400 / 1500: loss 6.352998\n",
      "training accuracy: 0.117376\n",
      "validation accuracy: 0.115714\n",
      "Got 1701 / 14700 correct => valid accuracy: 0.115714\n",
      "iteration 0 / 1500: loss 5.698857\n",
      "iteration 100 / 1500: loss 5.732406\n",
      "iteration 200 / 1500: loss 4.842404\n",
      "iteration 300 / 1500: loss 6.158812\n",
      "iteration 400 / 1500: loss 5.504429\n",
      "iteration 500 / 1500: loss 5.816861\n",
      "iteration 600 / 1500: loss 5.549920\n",
      "iteration 700 / 1500: loss 5.826445\n",
      "iteration 800 / 1500: loss 5.210573\n",
      "iteration 900 / 1500: loss 5.557461\n",
      "iteration 1000 / 1500: loss 5.748225\n",
      "iteration 1100 / 1500: loss 6.022713\n",
      "iteration 1200 / 1500: loss 5.512981\n",
      "iteration 1300 / 1500: loss 5.485673\n",
      "iteration 1400 / 1500: loss 5.787760\n",
      "training accuracy: 0.115539\n",
      "validation accuracy: 0.115714\n",
      "Got 1701 / 14700 correct => valid accuracy: 0.115714\n",
      "iteration 0 / 1500: loss 6.576563\n",
      "iteration 100 / 1500: loss 7.179721\n",
      "iteration 200 / 1500: loss 7.221714\n",
      "iteration 300 / 1500: loss 7.012483\n",
      "iteration 400 / 1500: loss 6.874966\n",
      "iteration 500 / 1500: loss 7.129919\n",
      "iteration 600 / 1500: loss 6.961970\n",
      "iteration 700 / 1500: loss 7.190002\n",
      "iteration 800 / 1500: loss 7.107840\n",
      "iteration 900 / 1500: loss 6.961582\n",
      "iteration 1000 / 1500: loss 7.455690\n",
      "iteration 1100 / 1500: loss 6.943080\n",
      "iteration 1200 / 1500: loss 7.360817\n",
      "iteration 1300 / 1500: loss 7.082889\n",
      "iteration 1400 / 1500: loss 7.160080\n",
      "training accuracy: 0.080292\n",
      "validation accuracy: 0.079796\n",
      "Got 1173 / 14700 correct => valid accuracy: 0.079796\n",
      "iteration 0 / 1500: loss 6.318392\n",
      "iteration 100 / 1500: loss 6.428383\n",
      "iteration 200 / 1500: loss 6.630575\n",
      "iteration 300 / 1500: loss 6.624755\n",
      "iteration 400 / 1500: loss 6.419079\n",
      "iteration 500 / 1500: loss 6.411661\n",
      "iteration 600 / 1500: loss 6.717462\n",
      "iteration 700 / 1500: loss 6.318892\n",
      "iteration 800 / 1500: loss 6.760517\n",
      "iteration 900 / 1500: loss 6.320131\n",
      "iteration 1000 / 1500: loss 6.569784\n",
      "iteration 1100 / 1500: loss 6.381502\n",
      "iteration 1200 / 1500: loss 6.336853\n",
      "iteration 1300 / 1500: loss 6.438089\n",
      "iteration 1400 / 1500: loss 6.114550\n",
      "training accuracy: 0.099708\n",
      "validation accuracy: 0.099116\n",
      "Got 1457 / 14700 correct => valid accuracy: 0.099116\n",
      "iteration 0 / 1500: loss 6.020490\n",
      "iteration 100 / 1500: loss 5.905455\n",
      "iteration 200 / 1500: loss 6.086323\n",
      "iteration 300 / 1500: loss 5.613775\n",
      "iteration 400 / 1500: loss 6.411104\n",
      "iteration 500 / 1500: loss 5.734570\n",
      "iteration 600 / 1500: loss 5.727561\n",
      "iteration 700 / 1500: loss 6.337665\n",
      "iteration 800 / 1500: loss 5.516329\n",
      "iteration 900 / 1500: loss 5.732913\n",
      "iteration 1000 / 1500: loss 5.779029\n",
      "iteration 1100 / 1500: loss 6.057449\n",
      "iteration 1200 / 1500: loss 5.813880\n",
      "iteration 1300 / 1500: loss 5.616418\n",
      "iteration 1400 / 1500: loss 5.546546\n",
      "training accuracy: 0.102624\n",
      "validation accuracy: 0.102041\n",
      "Got 1500 / 14700 correct => valid accuracy: 0.102041\n",
      "iteration 0 / 1500: loss 6.678880\n",
      "iteration 100 / 1500: loss 6.466076\n",
      "iteration 200 / 1500: loss 6.527440\n",
      "iteration 300 / 1500: loss 6.664397\n",
      "iteration 400 / 1500: loss 6.239475\n",
      "iteration 500 / 1500: loss 6.267925\n",
      "iteration 600 / 1500: loss 6.103507\n",
      "iteration 700 / 1500: loss 6.538665\n",
      "iteration 800 / 1500: loss 6.629457\n",
      "iteration 900 / 1500: loss 6.487460\n",
      "iteration 1000 / 1500: loss 6.623136\n",
      "iteration 1100 / 1500: loss 6.464071\n",
      "iteration 1200 / 1500: loss 6.161294\n",
      "iteration 1300 / 1500: loss 6.453228\n",
      "iteration 1400 / 1500: loss 6.402938\n",
      "training accuracy: 0.087493\n",
      "validation accuracy: 0.091565\n",
      "Got 1346 / 14700 correct => valid accuracy: 0.091565\n",
      "iteration 0 / 1500: loss 5.495591\n",
      "iteration 100 / 1500: loss 5.364979\n",
      "iteration 200 / 1500: loss 4.953223\n",
      "iteration 300 / 1500: loss 4.993532\n",
      "iteration 400 / 1500: loss 5.113556\n",
      "iteration 500 / 1500: loss 5.700577\n",
      "iteration 600 / 1500: loss 4.897024\n",
      "iteration 700 / 1500: loss 5.036215\n",
      "iteration 800 / 1500: loss 5.190006\n",
      "iteration 900 / 1500: loss 5.250453\n",
      "iteration 1000 / 1500: loss 5.363053\n",
      "iteration 1100 / 1500: loss 4.704807\n",
      "iteration 1200 / 1500: loss 5.456251\n",
      "iteration 1300 / 1500: loss 5.339189\n",
      "iteration 1400 / 1500: loss 5.313693\n",
      "training accuracy: 0.100612\n",
      "validation accuracy: 0.101769\n",
      "Got 1496 / 14700 correct => valid accuracy: 0.101769\n",
      "iteration 0 / 1500: loss 5.648986\n",
      "iteration 100 / 1500: loss 6.352334\n",
      "iteration 200 / 1500: loss 5.824492\n",
      "iteration 300 / 1500: loss 5.915111\n",
      "iteration 400 / 1500: loss 6.063844\n",
      "iteration 500 / 1500: loss 5.666655\n",
      "iteration 600 / 1500: loss 6.007811\n",
      "iteration 700 / 1500: loss 5.003817\n",
      "iteration 800 / 1500: loss 5.413339\n",
      "iteration 900 / 1500: loss 5.924845\n",
      "iteration 1000 / 1500: loss 5.414982\n",
      "iteration 1100 / 1500: loss 5.791856\n",
      "iteration 1200 / 1500: loss 5.742959\n",
      "iteration 1300 / 1500: loss 6.489131\n",
      "iteration 1400 / 1500: loss 5.299176\n",
      "training accuracy: 0.113936\n",
      "validation accuracy: 0.113197\n",
      "Got 1664 / 14700 correct => valid accuracy: 0.113197\n",
      "iteration 0 / 1500: loss 6.176633\n",
      "iteration 100 / 1500: loss 5.955741\n",
      "iteration 200 / 1500: loss 6.551283\n",
      "iteration 300 / 1500: loss 6.495392\n",
      "iteration 400 / 1500: loss 6.201792\n",
      "iteration 500 / 1500: loss 5.969375\n",
      "iteration 600 / 1500: loss 6.239315\n",
      "iteration 700 / 1500: loss 5.932289\n",
      "iteration 800 / 1500: loss 5.933700\n",
      "iteration 900 / 1500: loss 5.655222\n",
      "iteration 1000 / 1500: loss 6.181308\n",
      "iteration 1100 / 1500: loss 5.792606\n",
      "iteration 1200 / 1500: loss 6.007526\n",
      "iteration 1300 / 1500: loss 6.123616\n",
      "iteration 1400 / 1500: loss 6.252369\n",
      "training accuracy: 0.097230\n",
      "validation accuracy: 0.094014\n",
      "Got 1382 / 14700 correct => valid accuracy: 0.094014\n",
      "iteration 0 / 1500: loss 5.598500\n",
      "iteration 100 / 1500: loss 5.924857\n",
      "iteration 200 / 1500: loss 5.714708\n",
      "iteration 300 / 1500: loss 5.274108\n",
      "iteration 400 / 1500: loss 5.556432\n",
      "iteration 500 / 1500: loss 6.293154\n",
      "iteration 600 / 1500: loss 5.925000\n",
      "iteration 700 / 1500: loss 5.679393\n",
      "iteration 800 / 1500: loss 5.595723\n",
      "iteration 900 / 1500: loss 6.012413\n",
      "iteration 1000 / 1500: loss 5.855141\n",
      "iteration 1100 / 1500: loss 5.539638\n",
      "iteration 1200 / 1500: loss 5.453246\n",
      "iteration 1300 / 1500: loss 5.722113\n",
      "iteration 1400 / 1500: loss 5.570188\n",
      "training accuracy: 0.124140\n",
      "validation accuracy: 0.125646\n",
      "Got 1847 / 14700 correct => valid accuracy: 0.125646\n",
      "iteration 0 / 1500: loss 6.699803\n",
      "iteration 100 / 1500: loss 6.799226\n",
      "iteration 200 / 1500: loss 6.637060\n",
      "iteration 300 / 1500: loss 6.389829\n",
      "iteration 400 / 1500: loss 6.734430\n",
      "iteration 500 / 1500: loss 6.778494\n",
      "iteration 600 / 1500: loss 6.474284\n",
      "iteration 700 / 1500: loss 6.514894\n",
      "iteration 800 / 1500: loss 6.625873\n",
      "iteration 900 / 1500: loss 7.091901\n",
      "iteration 1000 / 1500: loss 6.486014\n",
      "iteration 1100 / 1500: loss 6.656256\n",
      "iteration 1200 / 1500: loss 7.141647\n",
      "iteration 1300 / 1500: loss 6.607213\n",
      "iteration 1400 / 1500: loss 6.233189\n",
      "training accuracy: 0.088251\n",
      "validation accuracy: 0.086259\n",
      "Got 1268 / 14700 correct => valid accuracy: 0.086259\n",
      "iteration 0 / 1500: loss 7.112131\n",
      "iteration 100 / 1500: loss 7.304730\n",
      "iteration 200 / 1500: loss 7.448664\n",
      "iteration 300 / 1500: loss 6.859583\n",
      "iteration 400 / 1500: loss 7.547070\n",
      "iteration 500 / 1500: loss 6.896280\n",
      "iteration 600 / 1500: loss 7.294613\n",
      "iteration 700 / 1500: loss 6.919147\n",
      "iteration 800 / 1500: loss 7.017515\n",
      "iteration 900 / 1500: loss 7.290176\n",
      "iteration 1000 / 1500: loss 6.923281\n",
      "iteration 1100 / 1500: loss 6.952440\n",
      "iteration 1200 / 1500: loss 7.105916\n",
      "iteration 1300 / 1500: loss 7.236182\n",
      "iteration 1400 / 1500: loss 6.882223\n",
      "training accuracy: 0.089504\n",
      "validation accuracy: 0.084966\n",
      "Got 1249 / 14700 correct => valid accuracy: 0.084966\n",
      "iteration 0 / 1500: loss 5.786506\n",
      "iteration 100 / 1500: loss 5.778594\n",
      "iteration 200 / 1500: loss 5.173513\n",
      "iteration 300 / 1500: loss 5.492912\n",
      "iteration 400 / 1500: loss 5.745172\n",
      "iteration 500 / 1500: loss 5.593795\n",
      "iteration 600 / 1500: loss 5.770983\n",
      "iteration 700 / 1500: loss 5.617844\n",
      "iteration 800 / 1500: loss 5.651911\n",
      "iteration 900 / 1500: loss 5.041908\n",
      "iteration 1000 / 1500: loss 5.645856\n",
      "iteration 1100 / 1500: loss 5.607162\n",
      "iteration 1200 / 1500: loss 5.620513\n",
      "iteration 1300 / 1500: loss 6.361120\n",
      "iteration 1400 / 1500: loss 5.699933\n",
      "training accuracy: 0.095627\n",
      "validation accuracy: 0.101293\n",
      "Got 1489 / 14700 correct => valid accuracy: 0.101293\n",
      "iteration 0 / 1500: loss 5.136757\n",
      "iteration 100 / 1500: loss 5.313427\n",
      "iteration 200 / 1500: loss 4.755297\n",
      "iteration 300 / 1500: loss 5.321398\n",
      "iteration 400 / 1500: loss 4.928948\n",
      "iteration 500 / 1500: loss 5.136754\n",
      "iteration 600 / 1500: loss 4.821953\n",
      "iteration 700 / 1500: loss 4.905701\n",
      "iteration 800 / 1500: loss 5.460872\n",
      "iteration 900 / 1500: loss 5.170358\n",
      "iteration 1000 / 1500: loss 4.825480\n",
      "iteration 1100 / 1500: loss 4.719648\n",
      "iteration 1200 / 1500: loss 4.658784\n",
      "iteration 1300 / 1500: loss 4.960076\n",
      "iteration 1400 / 1500: loss 4.699919\n",
      "training accuracy: 0.113994\n",
      "validation accuracy: 0.113946\n",
      "Got 1675 / 14700 correct => valid accuracy: 0.113946\n",
      "iteration 0 / 1500: loss 5.964650\n",
      "iteration 100 / 1500: loss 5.755434\n",
      "iteration 200 / 1500: loss 5.515440\n",
      "iteration 300 / 1500: loss 5.638386\n",
      "iteration 400 / 1500: loss 5.863458\n",
      "iteration 500 / 1500: loss 5.958100\n",
      "iteration 600 / 1500: loss 5.785964\n",
      "iteration 700 / 1500: loss 5.706466\n",
      "iteration 800 / 1500: loss 5.453528\n",
      "iteration 900 / 1500: loss 6.222348\n",
      "iteration 1000 / 1500: loss 6.055709\n",
      "iteration 1100 / 1500: loss 6.294271\n",
      "iteration 1200 / 1500: loss 5.894737\n",
      "iteration 1300 / 1500: loss 6.113924\n",
      "iteration 1400 / 1500: loss 5.689892\n",
      "training accuracy: 0.096997\n",
      "validation accuracy: 0.089728\n",
      "Got 1319 / 14700 correct => valid accuracy: 0.089728\n",
      "iteration 0 / 1500: loss 4.809237\n",
      "iteration 100 / 1500: loss 4.678399\n",
      "iteration 200 / 1500: loss 4.687035\n",
      "iteration 300 / 1500: loss 4.830852\n",
      "iteration 400 / 1500: loss 4.498191\n",
      "iteration 500 / 1500: loss 4.844952\n",
      "iteration 600 / 1500: loss 4.634315\n",
      "iteration 700 / 1500: loss 4.560148\n",
      "iteration 800 / 1500: loss 4.667409\n",
      "iteration 900 / 1500: loss 4.277133\n",
      "iteration 1000 / 1500: loss 4.598273\n",
      "iteration 1100 / 1500: loss 4.804272\n",
      "iteration 1200 / 1500: loss 4.785682\n",
      "iteration 1300 / 1500: loss 4.590507\n",
      "iteration 1400 / 1500: loss 4.626216\n",
      "training accuracy: 0.149883\n",
      "validation accuracy: 0.150272\n",
      "Got 2209 / 14700 correct => valid accuracy: 0.150272\n",
      "iteration 0 / 1500: loss 5.528822\n",
      "iteration 100 / 1500: loss 5.906268\n",
      "iteration 200 / 1500: loss 5.695519\n",
      "iteration 300 / 1500: loss 5.555096\n",
      "iteration 400 / 1500: loss 5.791831\n",
      "iteration 500 / 1500: loss 5.719016\n",
      "iteration 600 / 1500: loss 5.804923\n",
      "iteration 700 / 1500: loss 5.795000\n",
      "iteration 800 / 1500: loss 5.759189\n",
      "iteration 900 / 1500: loss 5.444369\n",
      "iteration 1000 / 1500: loss 5.405211\n",
      "iteration 1100 / 1500: loss 5.585053\n",
      "iteration 1200 / 1500: loss 5.846693\n",
      "iteration 1300 / 1500: loss 6.127400\n",
      "iteration 1400 / 1500: loss 5.532928\n",
      "training accuracy: 0.094636\n",
      "validation accuracy: 0.095646\n",
      "Got 1406 / 14700 correct => valid accuracy: 0.095646\n",
      "iteration 0 / 1500: loss 5.843625\n",
      "iteration 100 / 1500: loss 5.744874\n",
      "iteration 200 / 1500: loss 5.886515\n",
      "iteration 300 / 1500: loss 5.833311\n",
      "iteration 400 / 1500: loss 6.009579\n",
      "iteration 500 / 1500: loss 5.739188\n",
      "iteration 600 / 1500: loss 5.708523\n",
      "iteration 700 / 1500: loss 5.900069\n",
      "iteration 800 / 1500: loss 5.197823\n",
      "iteration 900 / 1500: loss 6.018899\n",
      "iteration 1000 / 1500: loss 5.608680\n",
      "iteration 1100 / 1500: loss 5.621562\n",
      "iteration 1200 / 1500: loss 5.945625\n",
      "iteration 1300 / 1500: loss 5.182316\n",
      "iteration 1400 / 1500: loss 5.564479\n",
      "training accuracy: 0.100583\n",
      "validation accuracy: 0.100000\n",
      "Got 1470 / 14700 correct => valid accuracy: 0.100000\n",
      "iteration 0 / 1500: loss 6.791258\n",
      "iteration 100 / 1500: loss 6.581877\n",
      "iteration 200 / 1500: loss 6.515425\n",
      "iteration 300 / 1500: loss 6.667606\n",
      "iteration 400 / 1500: loss 7.248672\n",
      "iteration 500 / 1500: loss 6.555173\n",
      "iteration 600 / 1500: loss 6.196436\n",
      "iteration 700 / 1500: loss 6.635578\n",
      "iteration 800 / 1500: loss 6.495758\n",
      "iteration 900 / 1500: loss 6.559726\n",
      "iteration 1000 / 1500: loss 6.597507\n",
      "iteration 1100 / 1500: loss 6.645534\n",
      "iteration 1200 / 1500: loss 6.623498\n",
      "iteration 1300 / 1500: loss 6.141587\n",
      "iteration 1400 / 1500: loss 6.465968\n",
      "training accuracy: 0.073644\n",
      "validation accuracy: 0.075714\n",
      "Got 1113 / 14700 correct => valid accuracy: 0.075714\n",
      "iteration 0 / 1500: loss 6.264855\n",
      "iteration 100 / 1500: loss 6.223074\n",
      "iteration 200 / 1500: loss 6.669359\n",
      "iteration 300 / 1500: loss 6.706306\n",
      "iteration 400 / 1500: loss 6.559764\n",
      "iteration 500 / 1500: loss 6.077784\n",
      "iteration 600 / 1500: loss 6.324842\n",
      "iteration 700 / 1500: loss 6.508742\n",
      "iteration 800 / 1500: loss 6.732370\n",
      "iteration 900 / 1500: loss 6.512938\n",
      "iteration 1000 / 1500: loss 6.498896\n",
      "iteration 1100 / 1500: loss 6.525959\n",
      "iteration 1200 / 1500: loss 6.172281\n",
      "iteration 1300 / 1500: loss 6.701876\n",
      "iteration 1400 / 1500: loss 5.867279\n",
      "training accuracy: 0.110408\n",
      "validation accuracy: 0.104354\n",
      "Got 1534 / 14700 correct => valid accuracy: 0.104354\n",
      "iteration 0 / 1500: loss 5.822449\n",
      "iteration 100 / 1500: loss 5.633809\n",
      "iteration 200 / 1500: loss 5.520519\n",
      "iteration 300 / 1500: loss 5.780598\n",
      "iteration 400 / 1500: loss 5.172537\n",
      "iteration 500 / 1500: loss 5.488319\n",
      "iteration 600 / 1500: loss 5.482573\n",
      "iteration 700 / 1500: loss 5.677005\n",
      "iteration 800 / 1500: loss 5.941857\n",
      "iteration 900 / 1500: loss 5.748220\n",
      "iteration 1000 / 1500: loss 5.673040\n",
      "iteration 1100 / 1500: loss 5.456548\n",
      "iteration 1200 / 1500: loss 5.876916\n",
      "iteration 1300 / 1500: loss 5.664375\n",
      "iteration 1400 / 1500: loss 5.551504\n",
      "training accuracy: 0.094927\n",
      "validation accuracy: 0.098639\n",
      "Got 1450 / 14700 correct => valid accuracy: 0.098639\n",
      "iteration 0 / 1500: loss 6.658743\n",
      "iteration 100 / 1500: loss 6.209911\n",
      "iteration 200 / 1500: loss 6.310456\n",
      "iteration 300 / 1500: loss 6.931398\n",
      "iteration 400 / 1500: loss 6.949932\n",
      "iteration 500 / 1500: loss 6.478749\n",
      "iteration 600 / 1500: loss 6.814417\n",
      "iteration 700 / 1500: loss 6.669048\n",
      "iteration 800 / 1500: loss 6.454322\n",
      "iteration 900 / 1500: loss 7.320087\n",
      "iteration 1000 / 1500: loss 6.910222\n",
      "iteration 1100 / 1500: loss 6.546350\n",
      "iteration 1200 / 1500: loss 6.692607\n",
      "iteration 1300 / 1500: loss 6.902400\n",
      "iteration 1400 / 1500: loss 6.453883\n",
      "training accuracy: 0.091108\n",
      "validation accuracy: 0.093197\n",
      "Got 1370 / 14700 correct => valid accuracy: 0.093197\n",
      "iteration 0 / 1500: loss 4.705097\n",
      "iteration 100 / 1500: loss 5.161423\n",
      "iteration 200 / 1500: loss 5.336869\n",
      "iteration 300 / 1500: loss 5.594349\n",
      "iteration 400 / 1500: loss 5.450816\n",
      "iteration 500 / 1500: loss 5.345824\n",
      "iteration 600 / 1500: loss 4.762674\n",
      "iteration 700 / 1500: loss 5.085676\n",
      "iteration 800 / 1500: loss 4.788567\n",
      "iteration 900 / 1500: loss 5.050691\n",
      "iteration 1000 / 1500: loss 5.378807\n",
      "iteration 1100 / 1500: loss 4.859128\n",
      "iteration 1200 / 1500: loss 5.272005\n",
      "iteration 1300 / 1500: loss 4.849539\n",
      "iteration 1400 / 1500: loss 4.986969\n",
      "training accuracy: 0.143324\n",
      "validation accuracy: 0.142925\n",
      "Got 2101 / 14700 correct => valid accuracy: 0.142925\n",
      "iteration 0 / 1500: loss 5.862772\n",
      "iteration 100 / 1500: loss 5.424070\n",
      "iteration 200 / 1500: loss 5.739411\n",
      "iteration 300 / 1500: loss 5.371431\n",
      "iteration 400 / 1500: loss 5.836066\n",
      "iteration 500 / 1500: loss 5.738019\n",
      "iteration 600 / 1500: loss 5.759107\n",
      "iteration 700 / 1500: loss 5.289304\n",
      "iteration 800 / 1500: loss 5.737547\n",
      "iteration 900 / 1500: loss 5.755439\n",
      "iteration 1000 / 1500: loss 5.707090\n",
      "iteration 1100 / 1500: loss 5.746254\n",
      "iteration 1200 / 1500: loss 5.884261\n",
      "iteration 1300 / 1500: loss 5.709802\n",
      "iteration 1400 / 1500: loss 5.785547\n",
      "training accuracy: 0.091953\n",
      "validation accuracy: 0.089184\n",
      "Got 1311 / 14700 correct => valid accuracy: 0.089184\n",
      "iteration 0 / 1500: loss 5.719969\n",
      "iteration 100 / 1500: loss 5.927886\n",
      "iteration 200 / 1500: loss 4.935022\n",
      "iteration 300 / 1500: loss 5.158377\n",
      "iteration 400 / 1500: loss 5.233922\n",
      "iteration 500 / 1500: loss 5.543675\n",
      "iteration 600 / 1500: loss 5.347422\n",
      "iteration 700 / 1500: loss 5.438494\n",
      "iteration 800 / 1500: loss 5.216502\n",
      "iteration 900 / 1500: loss 5.415610\n",
      "iteration 1000 / 1500: loss 5.323923\n",
      "iteration 1100 / 1500: loss 5.528866\n",
      "iteration 1200 / 1500: loss 5.704975\n",
      "iteration 1300 / 1500: loss 5.542500\n",
      "iteration 1400 / 1500: loss 6.105550\n",
      "training accuracy: 0.104169\n",
      "validation accuracy: 0.103878\n",
      "Got 1527 / 14700 correct => valid accuracy: 0.103878\n",
      "iteration 0 / 1500: loss 5.951479\n",
      "iteration 100 / 1500: loss 5.598340\n",
      "iteration 200 / 1500: loss 6.294223\n",
      "iteration 300 / 1500: loss 5.714697\n",
      "iteration 400 / 1500: loss 6.044911\n",
      "iteration 500 / 1500: loss 5.962090\n",
      "iteration 600 / 1500: loss 6.256220\n",
      "iteration 700 / 1500: loss 5.576465\n",
      "iteration 800 / 1500: loss 5.829239\n",
      "iteration 900 / 1500: loss 5.933736\n",
      "iteration 1000 / 1500: loss 5.751648\n",
      "iteration 1100 / 1500: loss 5.814715\n",
      "iteration 1200 / 1500: loss 5.426331\n",
      "iteration 1300 / 1500: loss 5.851000\n",
      "iteration 1400 / 1500: loss 5.924112\n",
      "training accuracy: 0.101312\n",
      "validation accuracy: 0.099252\n",
      "Got 1459 / 14700 correct => valid accuracy: 0.099252\n",
      "iteration 0 / 1500: loss 6.396023\n",
      "iteration 100 / 1500: loss 5.899749\n",
      "iteration 200 / 1500: loss 6.082114\n",
      "iteration 300 / 1500: loss 6.039287\n",
      "iteration 400 / 1500: loss 5.610625\n",
      "iteration 500 / 1500: loss 6.536419\n",
      "iteration 600 / 1500: loss 6.554848\n",
      "iteration 700 / 1500: loss 5.653808\n",
      "iteration 800 / 1500: loss 6.082941\n",
      "iteration 900 / 1500: loss 6.458457\n",
      "iteration 1000 / 1500: loss 6.116622\n",
      "iteration 1100 / 1500: loss 6.042447\n",
      "iteration 1200 / 1500: loss 5.880340\n",
      "iteration 1300 / 1500: loss 5.961650\n",
      "iteration 1400 / 1500: loss 5.942660\n",
      "training accuracy: 0.099300\n",
      "validation accuracy: 0.103673\n",
      "Got 1524 / 14700 correct => valid accuracy: 0.103673\n",
      "iteration 0 / 1500: loss 7.525634\n",
      "iteration 100 / 1500: loss 6.951054\n",
      "iteration 200 / 1500: loss 7.042484\n",
      "iteration 300 / 1500: loss 6.905475\n",
      "iteration 400 / 1500: loss 6.759850\n",
      "iteration 500 / 1500: loss 6.883811\n",
      "iteration 600 / 1500: loss 6.831506\n",
      "iteration 700 / 1500: loss 6.686138\n",
      "iteration 800 / 1500: loss 7.212382\n",
      "iteration 900 / 1500: loss 6.721228\n",
      "iteration 1000 / 1500: loss 7.030223\n",
      "iteration 1100 / 1500: loss 6.967760\n",
      "iteration 1200 / 1500: loss 7.243114\n",
      "iteration 1300 / 1500: loss 6.938463\n",
      "iteration 1400 / 1500: loss 6.853056\n",
      "training accuracy: 0.104810\n",
      "validation accuracy: 0.111088\n",
      "Got 1633 / 14700 correct => valid accuracy: 0.111088\n",
      "iteration 0 / 1500: loss 6.816718\n",
      "iteration 100 / 1500: loss 6.214738\n",
      "iteration 200 / 1500: loss 6.694952\n",
      "iteration 300 / 1500: loss 6.383248\n",
      "iteration 400 / 1500: loss 6.219234\n",
      "iteration 500 / 1500: loss 6.937794\n",
      "iteration 600 / 1500: loss 6.422203\n",
      "iteration 700 / 1500: loss 6.620989\n",
      "iteration 800 / 1500: loss 7.036939\n",
      "iteration 900 / 1500: loss 6.449499\n",
      "iteration 1000 / 1500: loss 6.546759\n",
      "iteration 1100 / 1500: loss 6.616183\n",
      "iteration 1200 / 1500: loss 6.330292\n",
      "iteration 1300 / 1500: loss 7.133415\n",
      "iteration 1400 / 1500: loss 6.506782\n",
      "training accuracy: 0.077813\n",
      "validation accuracy: 0.076327\n",
      "Got 1122 / 14700 correct => valid accuracy: 0.076327\n",
      "iteration 0 / 1500: loss 5.727486\n",
      "iteration 100 / 1500: loss 5.309036\n",
      "iteration 200 / 1500: loss 5.429765\n",
      "iteration 300 / 1500: loss 5.313521\n",
      "iteration 400 / 1500: loss 5.373379\n",
      "iteration 500 / 1500: loss 5.336278\n",
      "iteration 600 / 1500: loss 5.928892\n",
      "iteration 700 / 1500: loss 5.532190\n",
      "iteration 800 / 1500: loss 5.912530\n",
      "iteration 900 / 1500: loss 5.304577\n",
      "iteration 1000 / 1500: loss 5.636243\n",
      "iteration 1100 / 1500: loss 5.628136\n",
      "iteration 1200 / 1500: loss 5.172753\n",
      "iteration 1300 / 1500: loss 5.313598\n",
      "iteration 1400 / 1500: loss 6.133035\n",
      "training accuracy: 0.100292\n",
      "validation accuracy: 0.101905\n",
      "Got 1498 / 14700 correct => valid accuracy: 0.101905\n",
      "iteration 0 / 1500: loss 5.702939\n",
      "iteration 100 / 1500: loss 5.502240\n",
      "iteration 200 / 1500: loss 5.638799\n",
      "iteration 300 / 1500: loss 5.400206\n",
      "iteration 400 / 1500: loss 5.455221\n",
      "iteration 500 / 1500: loss 5.406921\n",
      "iteration 600 / 1500: loss 5.665922\n",
      "iteration 700 / 1500: loss 5.094728\n",
      "iteration 800 / 1500: loss 5.758988\n",
      "iteration 900 / 1500: loss 5.574816\n",
      "iteration 1000 / 1500: loss 5.308452\n",
      "iteration 1100 / 1500: loss 5.778697\n",
      "iteration 1200 / 1500: loss 4.995665\n",
      "iteration 1300 / 1500: loss 5.510461\n",
      "iteration 1400 / 1500: loss 5.653423\n",
      "training accuracy: 0.107318\n",
      "validation accuracy: 0.107891\n",
      "Got 1586 / 14700 correct => valid accuracy: 0.107891\n",
      "iteration 0 / 1500: loss 5.637045\n",
      "iteration 100 / 1500: loss 5.563949\n",
      "iteration 200 / 1500: loss 5.437284\n",
      "iteration 300 / 1500: loss 5.805764\n",
      "iteration 400 / 1500: loss 5.688763\n",
      "iteration 500 / 1500: loss 5.857171\n",
      "iteration 600 / 1500: loss 5.799686\n",
      "iteration 700 / 1500: loss 5.998114\n",
      "iteration 800 / 1500: loss 5.817765\n",
      "iteration 900 / 1500: loss 5.735714\n",
      "iteration 1000 / 1500: loss 5.700387\n",
      "iteration 1100 / 1500: loss 5.705752\n",
      "iteration 1200 / 1500: loss 5.297001\n",
      "iteration 1300 / 1500: loss 5.324771\n",
      "iteration 1400 / 1500: loss 5.329567\n",
      "training accuracy: 0.115685\n",
      "validation accuracy: 0.114694\n",
      "Got 1686 / 14700 correct => valid accuracy: 0.114694\n",
      "iteration 0 / 1500: loss 5.681175\n",
      "iteration 100 / 1500: loss 5.890399\n",
      "iteration 200 / 1500: loss 6.067824\n",
      "iteration 300 / 1500: loss 5.584912\n",
      "iteration 400 / 1500: loss 5.415382\n",
      "iteration 500 / 1500: loss 5.758320\n",
      "iteration 600 / 1500: loss 5.432352\n",
      "iteration 700 / 1500: loss 5.809281\n",
      "iteration 800 / 1500: loss 6.178122\n",
      "iteration 900 / 1500: loss 5.751166\n",
      "iteration 1000 / 1500: loss 5.496331\n",
      "iteration 1100 / 1500: loss 5.964737\n",
      "iteration 1200 / 1500: loss 6.250404\n",
      "iteration 1300 / 1500: loss 6.168406\n",
      "iteration 1400 / 1500: loss 5.397919\n",
      "training accuracy: 0.118134\n",
      "validation accuracy: 0.113878\n",
      "Got 1674 / 14700 correct => valid accuracy: 0.113878\n",
      "iteration 0 / 1500: loss 6.391216\n",
      "iteration 100 / 1500: loss 6.126738\n",
      "iteration 200 / 1500: loss 6.245059\n",
      "iteration 300 / 1500: loss 6.421160\n",
      "iteration 400 / 1500: loss 6.726899\n",
      "iteration 500 / 1500: loss 6.068020\n",
      "iteration 600 / 1500: loss 6.447344\n",
      "iteration 700 / 1500: loss 6.337312\n",
      "iteration 800 / 1500: loss 6.362323\n",
      "iteration 900 / 1500: loss 6.469347\n",
      "iteration 1000 / 1500: loss 6.394632\n",
      "iteration 1100 / 1500: loss 5.785007\n",
      "iteration 1200 / 1500: loss 6.578206\n",
      "iteration 1300 / 1500: loss 6.922402\n",
      "iteration 1400 / 1500: loss 5.995981\n",
      "training accuracy: 0.093557\n",
      "validation accuracy: 0.096395\n",
      "Got 1417 / 14700 correct => valid accuracy: 0.096395\n",
      "iteration 0 / 1500: loss 6.212722\n",
      "iteration 100 / 1500: loss 6.555366\n",
      "iteration 200 / 1500: loss 5.907345\n",
      "iteration 300 / 1500: loss 6.150742\n",
      "iteration 400 / 1500: loss 5.945077\n",
      "iteration 500 / 1500: loss 6.122321\n",
      "iteration 600 / 1500: loss 5.862573\n",
      "iteration 700 / 1500: loss 5.939165\n",
      "iteration 800 / 1500: loss 5.915902\n",
      "iteration 900 / 1500: loss 5.789947\n",
      "iteration 1000 / 1500: loss 6.092029\n",
      "iteration 1100 / 1500: loss 5.890022\n",
      "iteration 1200 / 1500: loss 5.649893\n",
      "iteration 1300 / 1500: loss 5.943342\n",
      "iteration 1400 / 1500: loss 5.855380\n",
      "training accuracy: 0.084490\n",
      "validation accuracy: 0.085850\n",
      "Got 1262 / 14700 correct => valid accuracy: 0.085850\n",
      "iteration 0 / 1500: loss 6.833814\n",
      "iteration 100 / 1500: loss 6.807999\n",
      "iteration 200 / 1500: loss 6.547200\n",
      "iteration 300 / 1500: loss 6.696866\n",
      "iteration 400 / 1500: loss 6.681023\n",
      "iteration 500 / 1500: loss 6.421051\n",
      "iteration 600 / 1500: loss 6.600104\n",
      "iteration 700 / 1500: loss 6.741953\n",
      "iteration 800 / 1500: loss 6.862879\n",
      "iteration 900 / 1500: loss 6.046297\n",
      "iteration 1000 / 1500: loss 6.368829\n",
      "iteration 1100 / 1500: loss 6.774477\n",
      "iteration 1200 / 1500: loss 6.037043\n",
      "iteration 1300 / 1500: loss 6.531971\n",
      "iteration 1400 / 1500: loss 6.589577\n",
      "training accuracy: 0.104431\n",
      "validation accuracy: 0.101497\n",
      "Got 1492 / 14700 correct => valid accuracy: 0.101497\n",
      "iteration 0 / 1500: loss 6.618183\n",
      "iteration 100 / 1500: loss 6.289852\n",
      "iteration 200 / 1500: loss 7.031186\n",
      "iteration 300 / 1500: loss 6.343068\n",
      "iteration 400 / 1500: loss 6.108492\n",
      "iteration 500 / 1500: loss 6.292724\n",
      "iteration 600 / 1500: loss 6.469997\n",
      "iteration 700 / 1500: loss 6.269121\n",
      "iteration 800 / 1500: loss 6.443997\n",
      "iteration 900 / 1500: loss 6.427067\n",
      "iteration 1000 / 1500: loss 6.227895\n",
      "iteration 1100 / 1500: loss 6.146864\n",
      "iteration 1200 / 1500: loss 6.490439\n",
      "iteration 1300 / 1500: loss 6.108681\n",
      "iteration 1400 / 1500: loss 6.588556\n",
      "training accuracy: 0.062974\n",
      "validation accuracy: 0.058435\n",
      "Got 859 / 14700 correct => valid accuracy: 0.058435\n",
      "iteration 0 / 1500: loss 5.934186\n",
      "iteration 100 / 1500: loss 5.848154\n",
      "iteration 200 / 1500: loss 5.484694\n",
      "iteration 300 / 1500: loss 5.452030\n",
      "iteration 400 / 1500: loss 5.166209\n",
      "iteration 500 / 1500: loss 5.424912\n",
      "iteration 600 / 1500: loss 5.814554\n",
      "iteration 700 / 1500: loss 5.780353\n",
      "iteration 800 / 1500: loss 5.503733\n",
      "iteration 900 / 1500: loss 5.622982\n",
      "iteration 1000 / 1500: loss 5.678341\n",
      "iteration 1100 / 1500: loss 5.533297\n",
      "iteration 1200 / 1500: loss 5.788632\n",
      "iteration 1300 / 1500: loss 5.597291\n",
      "iteration 1400 / 1500: loss 5.527986\n",
      "training accuracy: 0.101720\n",
      "validation accuracy: 0.103741\n",
      "Got 1525 / 14700 correct => valid accuracy: 0.103741\n",
      "iteration 0 / 1500: loss 6.346673\n",
      "iteration 100 / 1500: loss 6.895026\n",
      "iteration 200 / 1500: loss 6.665918\n",
      "iteration 300 / 1500: loss 6.301600\n",
      "iteration 400 / 1500: loss 6.499037\n",
      "iteration 500 / 1500: loss 5.974299\n",
      "iteration 600 / 1500: loss 6.625662\n",
      "iteration 700 / 1500: loss 6.353813\n",
      "iteration 800 / 1500: loss 5.800303\n",
      "iteration 900 / 1500: loss 6.469704\n",
      "iteration 1000 / 1500: loss 6.229820\n",
      "iteration 1100 / 1500: loss 6.820151\n",
      "iteration 1200 / 1500: loss 6.261504\n",
      "iteration 1300 / 1500: loss 6.440373\n",
      "iteration 1400 / 1500: loss 5.923350\n",
      "training accuracy: 0.090583\n",
      "validation accuracy: 0.087211\n",
      "Got 1282 / 14700 correct => valid accuracy: 0.087211\n",
      "iteration 0 / 1500: loss 5.971683\n",
      "iteration 100 / 1500: loss 5.670034\n",
      "iteration 200 / 1500: loss 5.393467\n",
      "iteration 300 / 1500: loss 5.783773\n",
      "iteration 400 / 1500: loss 5.575885\n",
      "iteration 500 / 1500: loss 6.020347\n",
      "iteration 600 / 1500: loss 6.071430\n",
      "iteration 700 / 1500: loss 5.858423\n",
      "iteration 800 / 1500: loss 5.903701\n",
      "iteration 900 / 1500: loss 5.619115\n",
      "iteration 1000 / 1500: loss 5.839129\n",
      "iteration 1100 / 1500: loss 5.010580\n",
      "iteration 1200 / 1500: loss 5.627157\n",
      "iteration 1300 / 1500: loss 5.834759\n",
      "iteration 1400 / 1500: loss 5.547596\n",
      "training accuracy: 0.102303\n",
      "validation accuracy: 0.104830\n",
      "Got 1541 / 14700 correct => valid accuracy: 0.104830\n",
      "iteration 0 / 1500: loss 5.173029\n",
      "iteration 100 / 1500: loss 5.007539\n",
      "iteration 200 / 1500: loss 5.305507\n",
      "iteration 300 / 1500: loss 5.086842\n",
      "iteration 400 / 1500: loss 5.174388\n",
      "iteration 500 / 1500: loss 5.012113\n",
      "iteration 600 / 1500: loss 5.217110\n",
      "iteration 700 / 1500: loss 4.888664\n",
      "iteration 800 / 1500: loss 4.997220\n",
      "iteration 900 / 1500: loss 5.077165\n",
      "iteration 1000 / 1500: loss 5.291750\n",
      "iteration 1100 / 1500: loss 4.958409\n",
      "iteration 1200 / 1500: loss 4.829448\n",
      "iteration 1300 / 1500: loss 5.512089\n",
      "iteration 1400 / 1500: loss 5.279311\n",
      "training accuracy: 0.125569\n",
      "validation accuracy: 0.118231\n",
      "Got 1738 / 14700 correct => valid accuracy: 0.118231\n",
      "iteration 0 / 1500: loss 6.477119\n",
      "iteration 100 / 1500: loss 7.124564\n",
      "iteration 200 / 1500: loss 6.692319\n",
      "iteration 300 / 1500: loss 6.931026\n",
      "iteration 400 / 1500: loss 6.841755\n",
      "iteration 500 / 1500: loss 6.913906\n",
      "iteration 600 / 1500: loss 6.906101\n",
      "iteration 700 / 1500: loss 6.480424\n",
      "iteration 800 / 1500: loss 6.841161\n",
      "iteration 900 / 1500: loss 6.492939\n",
      "iteration 1000 / 1500: loss 6.703138\n",
      "iteration 1100 / 1500: loss 7.079480\n",
      "iteration 1200 / 1500: loss 7.176448\n",
      "iteration 1300 / 1500: loss 6.314584\n",
      "iteration 1400 / 1500: loss 6.651605\n",
      "training accuracy: 0.073207\n",
      "validation accuracy: 0.071088\n",
      "Got 1045 / 14700 correct => valid accuracy: 0.071088\n",
      "iteration 0 / 1500: loss 5.601270\n",
      "iteration 100 / 1500: loss 6.249061\n",
      "iteration 200 / 1500: loss 6.405731\n",
      "iteration 300 / 1500: loss 6.222484\n",
      "iteration 400 / 1500: loss 6.207551\n",
      "iteration 500 / 1500: loss 6.080278\n",
      "iteration 600 / 1500: loss 6.046096\n",
      "iteration 700 / 1500: loss 6.238357\n",
      "iteration 800 / 1500: loss 6.080848\n",
      "iteration 900 / 1500: loss 5.957080\n",
      "iteration 1000 / 1500: loss 6.275909\n",
      "iteration 1100 / 1500: loss 6.155473\n",
      "iteration 1200 / 1500: loss 5.636076\n",
      "iteration 1300 / 1500: loss 6.516115\n",
      "iteration 1400 / 1500: loss 5.807357\n",
      "training accuracy: 0.102828\n",
      "validation accuracy: 0.100340\n",
      "Got 1475 / 14700 correct => valid accuracy: 0.100340\n",
      "iteration 0 / 1500: loss 7.289505\n",
      "iteration 100 / 1500: loss 7.525923\n",
      "iteration 200 / 1500: loss 6.825619\n",
      "iteration 300 / 1500: loss 7.101659\n",
      "iteration 400 / 1500: loss 6.661573\n",
      "iteration 500 / 1500: loss 7.159921\n",
      "iteration 600 / 1500: loss 7.001723\n",
      "iteration 700 / 1500: loss 6.905731\n",
      "iteration 800 / 1500: loss 6.897774\n",
      "iteration 900 / 1500: loss 7.173697\n",
      "iteration 1000 / 1500: loss 6.667802\n",
      "iteration 1100 / 1500: loss 6.998049\n",
      "iteration 1200 / 1500: loss 6.954868\n",
      "iteration 1300 / 1500: loss 7.015640\n",
      "iteration 1400 / 1500: loss 6.832550\n",
      "training accuracy: 0.078105\n",
      "validation accuracy: 0.078503\n",
      "Got 1154 / 14700 correct => valid accuracy: 0.078503\n",
      "iteration 0 / 1500: loss 5.577394\n",
      "iteration 100 / 1500: loss 5.818765\n",
      "iteration 200 / 1500: loss 6.374158\n",
      "iteration 300 / 1500: loss 5.728282\n",
      "iteration 400 / 1500: loss 6.193544\n",
      "iteration 500 / 1500: loss 6.187050\n",
      "iteration 600 / 1500: loss 5.885438\n",
      "iteration 700 / 1500: loss 6.189373\n",
      "iteration 800 / 1500: loss 5.916642\n",
      "iteration 900 / 1500: loss 5.869735\n",
      "iteration 1000 / 1500: loss 5.982603\n",
      "iteration 1100 / 1500: loss 6.253538\n",
      "iteration 1200 / 1500: loss 5.812233\n",
      "iteration 1300 / 1500: loss 6.038945\n",
      "iteration 1400 / 1500: loss 6.179045\n",
      "training accuracy: 0.090087\n",
      "validation accuracy: 0.094694\n",
      "Got 1392 / 14700 correct => valid accuracy: 0.094694\n",
      "iteration 0 / 1500: loss 5.109168\n",
      "iteration 100 / 1500: loss 5.267309\n",
      "iteration 200 / 1500: loss 5.118864\n",
      "iteration 300 / 1500: loss 4.739122\n",
      "iteration 400 / 1500: loss 4.569542\n",
      "iteration 500 / 1500: loss 5.430581\n",
      "iteration 600 / 1500: loss 4.288636\n",
      "iteration 700 / 1500: loss 4.942911\n",
      "iteration 800 / 1500: loss 5.355960\n",
      "iteration 900 / 1500: loss 4.869352\n",
      "iteration 1000 / 1500: loss 4.957301\n",
      "iteration 1100 / 1500: loss 4.800427\n",
      "iteration 1200 / 1500: loss 4.784084\n",
      "iteration 1300 / 1500: loss 5.192616\n",
      "iteration 1400 / 1500: loss 5.310212\n",
      "training accuracy: 0.119271\n",
      "validation accuracy: 0.123061\n",
      "Got 1809 / 14700 correct => valid accuracy: 0.123061\n",
      "iteration 0 / 1500: loss 5.821938\n",
      "iteration 100 / 1500: loss 6.216097\n",
      "iteration 200 / 1500: loss 6.103051\n",
      "iteration 300 / 1500: loss 5.793328\n",
      "iteration 400 / 1500: loss 5.669104\n",
      "iteration 500 / 1500: loss 6.603828\n",
      "iteration 600 / 1500: loss 5.732971\n",
      "iteration 700 / 1500: loss 5.617022\n",
      "iteration 800 / 1500: loss 5.142913\n",
      "iteration 900 / 1500: loss 6.238924\n",
      "iteration 1000 / 1500: loss 5.720702\n",
      "iteration 1100 / 1500: loss 6.123946\n",
      "iteration 1200 / 1500: loss 5.660617\n",
      "iteration 1300 / 1500: loss 6.139900\n",
      "iteration 1400 / 1500: loss 6.181594\n",
      "training accuracy: 0.093907\n",
      "validation accuracy: 0.088503\n",
      "Got 1301 / 14700 correct => valid accuracy: 0.088503\n",
      "iteration 0 / 1500: loss 6.974832\n",
      "iteration 100 / 1500: loss 7.167016\n",
      "iteration 200 / 1500: loss 6.759000\n",
      "iteration 300 / 1500: loss 6.304034\n",
      "iteration 400 / 1500: loss 6.450162\n",
      "iteration 500 / 1500: loss 6.682016\n",
      "iteration 600 / 1500: loss 6.921654\n",
      "iteration 700 / 1500: loss 6.068857\n",
      "iteration 800 / 1500: loss 6.554141\n",
      "iteration 900 / 1500: loss 6.714791\n",
      "iteration 1000 / 1500: loss 6.377187\n",
      "iteration 1100 / 1500: loss 6.257862\n",
      "iteration 1200 / 1500: loss 7.196419\n",
      "iteration 1300 / 1500: loss 6.622453\n",
      "iteration 1400 / 1500: loss 6.786550\n",
      "training accuracy: 0.082595\n",
      "validation accuracy: 0.084354\n",
      "Got 1240 / 14700 correct => valid accuracy: 0.084354\n",
      "iteration 0 / 1500: loss 6.255833\n",
      "iteration 100 / 1500: loss 5.846568\n",
      "iteration 200 / 1500: loss 5.901995\n",
      "iteration 300 / 1500: loss 5.324407\n",
      "iteration 400 / 1500: loss 6.190321\n",
      "iteration 500 / 1500: loss 6.587457\n",
      "iteration 600 / 1500: loss 5.948084\n",
      "iteration 700 / 1500: loss 5.904046\n",
      "iteration 800 / 1500: loss 5.988487\n",
      "iteration 900 / 1500: loss 5.986349\n",
      "iteration 1000 / 1500: loss 6.426246\n",
      "iteration 1100 / 1500: loss 5.667707\n",
      "iteration 1200 / 1500: loss 5.837354\n",
      "iteration 1300 / 1500: loss 5.688398\n",
      "iteration 1400 / 1500: loss 5.727589\n",
      "training accuracy: 0.100641\n",
      "validation accuracy: 0.100816\n",
      "Got 1482 / 14700 correct => valid accuracy: 0.100816\n",
      "iteration 0 / 1500: loss 6.451053\n",
      "iteration 100 / 1500: loss 5.964619\n",
      "iteration 200 / 1500: loss 6.229079\n",
      "iteration 300 / 1500: loss 6.129247\n",
      "iteration 400 / 1500: loss 5.773410\n",
      "iteration 500 / 1500: loss 5.922167\n",
      "iteration 600 / 1500: loss 6.091427\n",
      "iteration 700 / 1500: loss 6.347767\n",
      "iteration 800 / 1500: loss 5.601676\n",
      "iteration 900 / 1500: loss 6.086067\n",
      "iteration 1000 / 1500: loss 6.035778\n",
      "iteration 1100 / 1500: loss 6.208872\n",
      "iteration 1200 / 1500: loss 5.881313\n",
      "iteration 1300 / 1500: loss 5.855759\n",
      "iteration 1400 / 1500: loss 5.988190\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "learning_rates = [10**(-f) for f in np.arange(3,6,0.2)]\n",
    "regularization_strengths = [10**(f) for f in np.arange(0.5,4,.4)]\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "pass\n",
    "\n",
    "\n",
    "X_t=[]\n",
    "y_t=[]\n",
    "num_tr=int(X_train.shape[0]*0.7)\n",
    "num_val=X_train.shape[0]-num_tr\n",
    "\n",
    "\n",
    "\n",
    "X_t=X_train[:num_tr,:]\n",
    "y_t=y_train[:num_tr,:]\n",
    "X_v=np.array(X_train[num_tr:,:])\n",
    "y_v=np.array(y_train[num_tr:,:])\n",
    "dim= X_train.shape[1]\n",
    "\n",
    "valv=X_v.size/dim\n",
    "X_v=X_v.reshape(valv,dim)\n",
    "c=0\n",
    "for l in learning_rates:\n",
    "    for r in regularization_strengths:\n",
    "        acc=0.0\n",
    "        \n",
    "        softmax = Softmax()\n",
    "\n",
    "        loss_hist = softmax.train(X_train, y_train, l, r,\n",
    "                      num_iters=1500, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_t)\n",
    "        train_acc=np.mean(y_t == y_train_pred)\n",
    "        print('training accuracy: %f' % (np.mean(y_t == y_train_pred), ))\n",
    "        y_val_pred = softmax.predict(X_v)\n",
    "        print('validation accuracy: %f' % (np.mean(y_v == y_val_pred), ))        \n",
    "        num_valid=(y_v.size)\n",
    "        # Compute and print the fraction of correctly predicted examples\n",
    "        num_correct = np.sum(y_v == y_val_pred)\n",
    "        accuracy = float(num_correct) / num_valid\n",
    "        print('Got %d / %d correct => valid accuracy: %f' % (num_correct, num_valid, accuracy))\n",
    "\n",
    "        results[(l,r)]=(train_acc,accuracy)\n",
    "        if accuracy>best_val:\n",
    "            best_val=accuracy\n",
    "            best_softmax=softmax\n",
    "            \n",
    "            \n",
    "    c+=1\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF8CAYAAADrUz6WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlCAIgiAIQhjIy5QgCIIgCEIY/B9Skra2QvmhIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f10893b9890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
